{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVtcdjt0Dwg-"
   },
   "source": [
    "# Exercise 1.1.1: Choosing Metrics\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHjcs4XyFUW_"
   },
   "source": [
    "## Objectives\n",
    "* Implement a function that calculates [IoU](https://en.wikipedia.org/wiki/Jaccard_index) scores between two bounding boxes;\n",
    "* Calculate [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) for a given set of prediction and the ground truth pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIWZzk8HGKUf"
   },
   "source": [
    "## 1. Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UsUS1hjlD-SW"
   },
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NBRytndeD-J2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sbsc28yGXlG"
   },
   "source": [
    "### 1.1. Calculating IoU\n",
    "\n",
    "Before we begin our implementation, let's review the definition of IoU from [Rezatofighi et al., 2018](https://giou.stanford.edu/) [1] and why we choose this metric for our bounding box prediction task.\n",
    "\n",
    "_Intersection over Union (IoU), also known as [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index), is the most popular evaluation metric for tasks such as segmentation, object detection and tracking. In object detection, IoU is used as a metric to evaluate how close the prediction and ground truth bounding boxes overlap._ \n",
    "\n",
    "_IoU has the appealing property of scale invariance. This means that the width, height and location of the two bounding boxes under consideration are taken into account. The normalized IoU measure focuses on the area of the shapes, no matter their size._\n",
    "\n",
    "In summary, IoU is a metric that describes the amount of overlap between two bounding boxes with a normalized score ranging from `0.0` to `1.0`. The greater the overlapping region, the greater our IoU score will be. The IoU score will be an important factor in determining the \"quality\" of our bounding box predictions, as the size of the overlapping regions of interest will vary with respect to the accuracy of our predicted coordinates.\n",
    "\n",
    "Illustrated in Fig. 1 is a set of possible bounding box configuratons. For our problem, we can consider one of the two boxes to be formed from the ground truth coordinates and the other to be formed from predicted coordinates. In the case of our problem, a full overlap shown in Fig. 1(d) represents a perfect prediction and is given an IoU score of `1.0`. On the other hand, zero overlap shown in Fig. 1(a) corresponds to an IoU score of `0.0` and usually indicates a complete mismatch between the predicted and ground truth bounding boxes.\n",
    "\n",
    "\n",
    "#### Possible bounding box configurations\n",
    "\n",
    "![Fig. 1. IoU - Four possible bounding boxes.](figures/2022-07-25-Figure-1-IoU-Boxes.jpg)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Fig. 1. } & \\textrm{Four possible bounding box pairs with highlighted regions of intersection (shown in black).} \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "#### Possible non-overlapping bounding boxes\n",
    "\n",
    "It will be important to note in our algorithm the four possible cases when the two bounding boxes _do not_ overlap, as illustrated in Fig. 1(a). There are four possible cases when two boxes may not overlap, corresponding to the locations of the respective left and right, upper and lower edges of boxes 1 and 2. \n",
    "\n",
    "These four cases of non-overlap are illustrated in Fig. 2 below and will be covered in more detail in the later steps of this assignment. Note that in our problem we define \"overlap\" to be any pair of intersecting rectangles with a non-zero area of intersection.\n",
    "\n",
    "![Fig. 2. IoU - Four possible bounding box overlaps.](figures/2022-07-25-Figure-2-IoU-Non-Overlapping-Boxes.jpg)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Fig. 2. } & \\textrm{Four possible bounding box pairs, each with zero overlap.} \\\\\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsVOLhpqEDQ9"
   },
   "source": [
    "#### Regions of interest\n",
    "\n",
    "In order to calculate IoU for our bounding box prediction problem, we need to identify several areas of interest. \n",
    "\n",
    "The first region we will calculate is the _area of intersection_ shown in Fig. 3(a). This is the region contained inside the overlapping predicted and ground truth bounding boxes.\n",
    "\n",
    "The second region we will calculate is the _area of union_ shown in Fig. 3(b). This is the total combined area of both boxes. Note that when calculating this area we will subtract off the inner-most sub-region, i.e., the area of intersection, to prevent it from contributing twice in our IoU formula (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig. 3. The two overlapping areas of interest (shown in black).](figures/2022-07-25-Figure-3-IoU-Regions-of-Interest.jpg)\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Fig. 3. } & \\textrm{The two overlapping areas of interest (shown in black).} \\\\\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have these two areas computed, we can the obtain the normalised IoU score derived from the formula illustrated below. The IoU score is simply the ratio between the two areas and is naturally normalised between `0.0` and `1.0`. \n",
    "\n",
    "\n",
    "![Fig. 4. The normalised IoU score formula.](figures/2022-07-25-Figure-4-IoU-Formula.jpg)\n",
    "\n",
    "$$\\textrm{Fig. 4. The normalised IoU score formula illustrated for the bounding box prediction problem.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating area of intersection\n",
    "\n",
    "In order to solve for the area of the inner region shown in the numerator of Fig. 4, recall the two points derived from our previous calculations above. That is, the coordinates $(x_3, y_3)$ and $(x_2, y_2)$ from Fig. 3. From these we can obtain the width, $w$, and height, $h$, of the intersection by taking the difference of the two $x$- and $y$- coordinate values,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    w_b & = \\left \\vert x_2 - x_3 \\right \\vert, \\\\\n",
    "    h_b & = \\left \\vert y_2 - y_3 \\right \\vert.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "From there we can find our _area of intersection_ value, which will serve as the numerator of the _IoU_ formula,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    Area_{i} & = w_b * h_b. \\\\\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating area of union\n",
    "\n",
    "The denominator of the IoU formula shown in Fig. 4 corresponds to the _total_ or combined area of both bounding boxes. We will work backwards to find this by first computing each box's area individually. For the upper-left box, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    w_1 & = \\left \\vert x_2 - x_1 \\right \\vert, \\\\\n",
    "    h_1 & = \\left \\vert y_2 - y_1 \\right \\vert, \\\\\n",
    "    Area_1 &= w_1 * h_1.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Repeating for the lower-right box, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    w_2 &= \\left \\vert x_4 - x_3 \\right \\vert, \\\\\n",
    "    h_2 &= \\left \\vert y_4 - y_3 \\right \\vert, \\\\\n",
    "    Area_2 &= w_2 * h_2.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "To obtain the area of union, we will then add the two area values we calculated. However, we must subtract off the area of the intersection found above since we only want to count it once,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    Area_{u} &= {Area_1 + Area_2} - {Area_i}. \\\\\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in computing the _areas_ of _intersection_ and _union_, we are going to need a few extra calculations. Luckily, these are very easy formulas to recall from elementary geometry. As we are working with rectangle bounding boxes, the area is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    Area &= \\left \\vert w * h \\right \\vert. \\\\\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the IoU score\n",
    "Last but not least, we will use the values we obtained above to fit the formula illustrated in Fig. 4,\n",
    "$$\n",
    "\\begin{align}\n",
    "    IoU &= \\frac{\\textrm{Area of Intersection}}{\\textrm{Area of Union}} = \\frac{Area_i}{Area_u}. \\\\\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzMOZXHwOOGM"
   },
   "source": [
    "Not convinced? We will look at one additional example specific to our problem. \n",
    "\n",
    "In the next section, we will cover the IoU formula applied to a more specific example — the bounding box prediction problem we are trying to solve. \n",
    "\n",
    "Before we move on, let's recap the steps above:\n",
    "\n",
    "1. Determine if two bounding boxes overlap;\n",
    "1. If so, identify the two areas of interest — _intersection_ and _union_;\n",
    "2. Compute their respective areas;\n",
    "3. Obtain the IoU score by calculating the ratio of the two areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. IoU for Bounding Box Prediction\n",
    "\n",
    "\n",
    "Now that you have the IoU formula down, let's consider a more practical example applied to the bounding box prediction problem we are trying to solve.\n",
    "\n",
    "![Fig. 5. A practical example of the IoU scoring algorithm applied to bounding box prediction for 2D object recognition.](figures/2022-07-25-Figure-5-IoU-Vehicle.jpg)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Fig. 5. } & \\textrm{A starting point for the IoU scoring algorithm applied to bounding box prediction for 2D object recognition.} \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Here shown in Fig. 5. we have a pair of bounding boxes labelled with their coordinate pairs. As shown in the figure legend, the solid green coloured box indicates the _ground truth_ bounding box, while the dashed red coloured box is the corresponding _predicted_ bounding box.\n",
    "\n",
    "\n",
    "In order to perform the IoU calculations described above, we will need to first identify the the upper-left and lower-right coordinates highlighted in yellow above. These coordinate values will be used in the later steps of our IoU algorithm to compute the area of intersection and in turn the IoU score. Before we get into the details of this important step, let's first review our input data and several key assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az92VJf9Fn28"
   },
   "source": [
    "#### Considerations for our input data\n",
    "\n",
    "Let's consider the data we are given as input to our `calculate_ious` function. The first list, `gt_bboxes`, contains `(x, y)`coordinate pairs corresponding to the upper-left and lower-right coordinates of each ground truth bounding box. The second list, `pred_bboxes`, is similar to the first, but contains instead the coordinate pairs of each _predicted_ bounding box. It is then our job to figure out the overlapping region of interest between each _ground truth_ and _predicted_ bounding box, as illustrated in Fig. 5 above. To do so, we need to know which values the coordinates $(x_3, y_3)$ and $(x_2, y_2)$ represent in our input lists. While this looks like a trivial exercise from the example above, we have to acknowledge a few important assumptions before we get into calculating the IoU forumla.\n",
    "\n",
    "#### Making key assumptions\n",
    "\n",
    "First assumption we make relates to our coordinate reference system. We will define the origin `(0,0)` to be the upper-left most corner of our image. Following this convention, the x-axis values will increase going right and the y-axis values will increase going down (as shown in Fig. 5).\n",
    "\n",
    "Our second assumption governs our input values in the `pred_bboxes` and `gt_bboxes` lists. Each coordinate pair is a set of two integer values `x` and `y` describing either the _upper-left_ or _lower-right_ most coordinates of each bounding box. For example, the following input lists\n",
    "```\n",
    "gt_bboxes = [x_1, y_1, x_2, y_2]\n",
    "pred_bboxes = [x_3, y_3, x_4, y_4]\n",
    "```\n",
    "describe an overlap of bounding boxes illustrated in Fig. 3.\n",
    "\n",
    "#### Obtaining the coordinates of the area of intersection\n",
    "Recall from Fig. 1 that these bounding boxes can overlap in many ways than one, or not overlap at all. \n",
    "\n",
    "To calculate the yellow-highlighted pair of coordinates describing the area of intersection in Fig. 5, we will make use of Python's built-in `min` and `max` formulas. \n",
    "\n",
    "Given the coordinate reference system we defined above, we will be using the `max` function over the two boxes' upper-left `x`- and `y`-coordinate values, and the `min` function over the lower-right `x`- and `y`-coordinate values, as such:\n",
    "```\n",
    "(x_b1, y_b1) := max(x_1, x_3), max(y_1, y_3)\n",
    "(x_b2, y_b2) := min(x_2, x_4), min(y_2, y_4)\n",
    "```\n",
    "\n",
    "```\n",
    "(x_inter1, y_inter1) := max(x_1, x_3), max(y_1, y_3)\n",
    "(x_inter2, y_inter2) := min(x_2, x_4), min(y_2, y_4)\n",
    "```\n",
    "The region of intersection is then described by the upper-left coordinates `(x_b1, y_b1)` and lower-right coordinates `(x_b2, y_b2)` which, from Fig. 3, are points `(x_3, y_3)` and `(x_2, y_2)` respectively.\n",
    "\n",
    "#### Calculating the areas of interest\n",
    "\n",
    "Now that we have obtained the respective coordinate pairs, we can then calculate the area of the two key regions of interest. \n",
    "\n",
    "Once we have these two areas computed using the formulas described in _Part 1_, we can proceed to obtain the normalised IoU score classifying the \"quality\" of the bounding box prediction relative to the ground truth.\n",
    "\n",
    "#### Determining the quality of predictions\n",
    "While the IoU score can be a pretty subjective measure of prediction accuracy, there are a few \"rules of thumb\" that interpret the IoU scores:\n",
    "* \"Very good\": $ 0.9 \\lt IoU \\leq 1.0$;\n",
    "* \"Good\": $0.7 \\lt IoU \\leq 0.9$;\n",
    "* \"Fair\": $0.5 \\lt IoU \\leq 0.7$.\n",
    "\n",
    "Any pair of bounding boxes with an $IoU$ score of less than $0.5$ are typically considered disjoint, i.e., boxes bounding _not_ belonging to the same object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiting the example in Fig. 4\n",
    "\n",
    "Still not sure you have the IoU algorithm down? No fret — let's go through it together step-by-step using real coordinate values. It's always a great idea to go through an algorithm line-by-line using test cases like the one illustrated in Fig. 5. Let's get started!\n",
    "\n",
    "\n",
    "From Fig. 5 we have two bounding boxes; the greeen solid line is our ground-truth bounding box, and the red dotted line is our predicted bounding box. Therefore, we can expect the following set of input data,\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{gt_bboxes} &= \\texttt{[2, 3, 13, 10]} \\\\\n",
    "    \\texttt{pred_bboxes} &= \\texttt{[3, 2, 13, 11]} \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "our goal is to then return the correspnding IoU score that assigns a sort-of accuracy metric to the bounding box prediction shown in red.\n",
    "\n",
    "The first step of our IoU algorithm is to find the coordinates of the area of intersection. Let's start by finding the upper-left x- and y-coordinates now —\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{x_inter1} &= \\texttt{max(gt_bboxes[0], pred_bboxes[0])} \\\\\n",
    "                      &= \\texttt{max(2, 3)} = \\textbf{3}, \\\\\n",
    "    \\texttt{y_inter1} &= \\texttt{max(gt_bboxes[1], pred_bboxes[1])} \\\\\n",
    "                      &= \\texttt{max(3, 2)} = \\textbf{3}. \\\\    \n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "We can verify that our result for the first coordinate pair `(x_inter1, y_inter1)` is indeed equal to `(3, 3)`, matching the highlighted upper-left coordinate pair shown in Fig. 5. \n",
    "\n",
    "Repeating for the lower-right x- and y-coordinates —\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{x_inter2} &= \\texttt{min(gt_bboxes[2], pred_bboxes[2])} \\\\\n",
    "                      &= \\texttt{min(13, 13)} = \\textbf{13}, \\\\\n",
    "    \\texttt{y_inter2} &= \\texttt{min(gt_bboxes[3], pred_bboxes[3])} \\\\\n",
    "                      &= \\texttt{min(10, 11)} = \\textbf{10}. \\\\    \n",
    "    \\end{align}\n",
    "$$\n",
    "Again, our second coordinate pair of `(13, 10)` checks out, matching the highlighted lower-right coordinate pair shown in Fig. 5. Awesome!\n",
    "\n",
    "\n",
    "The second step in our IoU algorithm is to calculate the areas of intersection. In order to do so, we first obtain the _width_ and _height_ of the rectangular region of interesection.\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{w_inter} &= \\texttt{abs(x_inter2 - x_inter1)} \\\\\n",
    "                     &= \\texttt{abs(13 - 3)} = \\textbf{10}, \\\\\n",
    "    \\texttt{h_inter} &= \\texttt{abs(y_inter2 - y_inter1)} \\\\\n",
    "                     &= \\texttt{abs(10 - 3)} = \\textbf{7}, \\\\  \n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Proceeding to calculate the area of the intersection from these values, we obtain\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{area_inter} &= \\texttt{w_inter * h_inter} \\\\\n",
    "                        &= \\texttt{10 * 7} = \\textbf{70}. \\\\  \n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Through visual inspection we can see that the value of $70$ we obtained for the area of intersection indeed matches what we see in Fig. 5.\n",
    "\n",
    "Moving on to the third step of our IoU algorithm: calculating the coordinates belonging to the area of union. We will start by fnding the _width_ and _height_ belonging to each of the two bounding boxes, then calculating their areas independently of each other —\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{w_union1} &= \\texttt{abs(gt_bboxes[2] - gt_bboxes[0])} \\\\\n",
    "                      &= \\texttt{abs(13 - 2)} = \\textbf{11}, \\\\\n",
    "    \\texttt{h_union1} &= \\texttt{abs(gt_bboxes[3] - gt_bboxes[1])} \\\\\n",
    "                      &= \\texttt{abs(10 - 3)} = \\textbf{7}, \\\\\n",
    "    \\texttt{area_union1} &= \\texttt{w_union1 * h_union1} \\\\\n",
    "                         &= \\texttt{11 * 7} = \\textbf{77}. \\\\  \n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The first area we obtain corresponds to the ground truth bounding box with a value of $77$. Repeating for the predicted bounding box,\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{w_union2} &= \\texttt{abs(pred_bboxes[2] - pred_bboxes[0])} \\\\\n",
    "                      &= \\texttt{abs(13 - 3)} = \\textbf{10}, \\\\\n",
    "    \\texttt{h_union2} &= \\texttt{abs(pred_bboxes[3] - pred_bboxes[1])} \\\\\n",
    "                      &= \\texttt{abs(11 - 2)} = \\textbf{9}, \\\\\n",
    "    \\texttt{area_union2} &= \\texttt{w_union2 * h_union2} \\\\\n",
    "                         &= \\texttt{10 * 9} = \\textbf{90}. \\\\  \n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "we obtain an value of $90$. To calculate the _area of union_, we will follow the formula from Sect. 1.1 to eliminate the duplicate overlapping region and leave us with the combined area of both boxes. In Fig. 5, this is done as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{area_union} &= \\texttt{(area_union1 + area_union2) - area_inter} \\\\\n",
    "                        &= \\texttt{(77 + 90) - 70} = \\textbf{97}. \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Lastly, we will compute the IoU score for the bounding box pair in order to obtain a prediction accuracy metric. This is simply the ratio of the area of intersection to area of union,\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\texttt{IoU} &= \\texttt{(area_inter) / (area_union)} \\\\\n",
    "                 &= \\texttt{(70) / (97)} \\\\\n",
    "                 &\\approx \\textbf{0.72}. \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Great — we have shown that our algorithm correctly computes the IoU score for the example shown in Fig. 5. Using the \"rule of thumb\" interpretation of the resulting IoU score of $0.72$ leaves us with a prediction accuracy classification of _good_ meaning that we can safely assume the predicted bounding box roughly encloses the true area of the vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IoU algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QUJtNLwkDxER"
   },
   "outputs": [],
   "source": [
    "### From Udacity's `iou.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoU:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _overlapping_rectangles(bbox1: List[int], bbox2: List[int]) -> bool:\n",
    "        \"\"\"Returns True if the bounding boxes overlap.\n",
    "        \n",
    "        Two bounding boxes overlap if their area is positive and non-zero.\n",
    "        \n",
    "        :param bbox1: 1x4 list of x-y coordinates forming a rectangle.\n",
    "        :param bbox2: 1x4 list of x-y coordinates forming a rectangle.\n",
    "        :returns: bool, whether or not the two rectangles overlap.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Check if bounding boxes do not overlap\n",
    "        if (\n",
    "            # (a) First bbox lower edge is GEQ second bbox upper edge\n",
    "            (min(bbox1[1], bbox2[1]) >= max(bbox1[3], bbox2[3])) or\n",
    "            # (b) First bbox right edge is LEQ second bbox left edge\n",
    "            (min(bbox1[2], bbox2[2]) <= max(bbox1[0], bbox2[0])) or\n",
    "            # (c) First bbox left edge is GEQ second bbox right edge\n",
    "            (min(bbox1[0], bbox2[0]) >= max(bbox1[2], bbox2[2])) or\n",
    "            # (d) First bbox upper edge is LEQ second bbox lower edge\n",
    "            (min(bbox1[3], bbox2[3]) <= max(bbox1[1], bbox2[1]))\n",
    "        ):\n",
    "            return False\n",
    "        # 2. Check if intersection area is larger than 0\n",
    "        else:\n",
    "            x_inter1 = max(bbox1[0], bbox2[0])\n",
    "            y_inter1 = max(bbox1[1], bbox2[1])\n",
    "            x_inter2 = min(bbox1[2], bbox2[2])\n",
    "            y_inter2 = min(bbox1[3], bbox2[3])\n",
    "            # Overlapping region must have positive area, if not intersect, then all 0.\n",
    "            w_inter = max(0, (x_inter2 - x_inter1))\n",
    "            h_inter = max(0, (y_inter2 - y_inter1))\n",
    "            if (w_inter * h_inter) > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _calculate_iou(self, gt_bbox: List[int], pred_bbox: List[int]) -> float:\n",
    "        \"\"\"Calculates the IoU score for a single pair of bounding boxes.\n",
    "\n",
    "        :param gt_bbox: 1x4 list of ground-truth coordinates,\n",
    "        :param pred_bbox: 1x4 list of predicted coordinates,\n",
    "        returns: iou, the pairwise IoU score between the two bounding boxes.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Check if bounding boxes overlap\n",
    "        if not self._overlapping_rectangles(gt_bbox, pred_bbox):\n",
    "            return 0.0\n",
    "        else:\n",
    "            # 2. Find the coordinates of the area of intersection\n",
    "            #    2a. The upper-left coordinate\n",
    "            x_inter1 = max(gt_bbox[0], pred_bbox[0])\n",
    "            y_inter1 = max(gt_bbox[1], pred_bbox[1])\n",
    "            #    2b. The lower-right coordinate\n",
    "            x_inter2 = min(gt_bbox[2], pred_bbox[2])\n",
    "            y_inter2 = min(gt_bbox[3], pred_bbox[3])\n",
    "            # 3. Calculate the area of intersection\n",
    "            w_inter = abs(x_inter2 - x_inter1)\n",
    "            h_inter = abs(y_inter2 - y_inter1)\n",
    "            area_inter = w_inter * h_inter\n",
    "            # 4. Find the coordinates of the area of union\n",
    "            #    4a. The height and width of the first box \n",
    "            w_union1 = abs(gt_bbox[2] - gt_bbox[0])\n",
    "            h_union1 = abs(gt_bbox[3] - gt_bbox[1])\n",
    "            #    4b. The height and width of the second box\n",
    "            w_union2 = abs(pred_bbox[2] - pred_bbox[0])\n",
    "            h_union2 = abs(pred_bbox[3] - pred_bbox[1])\n",
    "            # 5. Calculate the area of union\n",
    "            area_union = (w_union1 * h_union1) + (w_union2 * h_union2)\n",
    "            area_union -= area_inter\n",
    "            # 6. Calculate the resulting IoU score\n",
    "            iou = float(area_inter) / float(area_union)\n",
    "        return iou\n",
    "\n",
    "    def calculate_ious(self, gt_bboxes: List[List[int]], \n",
    "                       pred_bboxes: List[List[int]]) -> List[float]:\n",
    "        \"\"\"Calculates the IoU scores for all bounding box pairs.\n",
    "\n",
    "        :param gt_bboxes: Nx4 list of ground-truth coordinates,\n",
    "        :param pred_bboxes: Mx4 list of predicted coordinates,\n",
    "        returns: iou, NxM list of pairwise IoU scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Allocate a NumPy array of zeros to store the IoU scores\n",
    "        ious = np.zeros((gt_bboxes.shape[0], pred_bboxes.shape[0]))\n",
    "        # For each ground-truth bounding box\n",
    "        for i, gt_bbox in enumerate(gt_bboxes):\n",
    "            # Calculate the IoU score w.r.t the entire inference set\n",
    "            for j, pred_bbox in enumerate(pred_bboxes):\n",
    "                ious[i,j] = self._calculate_iou(gt_bbox, pred_bbox)\n",
    "        return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `solution/iou.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoUSolution:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    @staticmethod    \n",
    "    def _overlapping_rectangles(bbox1: List[int], bbox2: List[int]) -> bool:\n",
    "        \"\"\"Returns True if the bounding boxes overlap.\n",
    "        \n",
    "        Two bounding boxes overlap if their area is positive and non-zero.\n",
    "        \n",
    "        :param bbox1: 1x4 list of x-y coordinates forming a rectangle.\n",
    "        :param bbox2: 1x4 list of x-y coordinates forming a rectangle.\n",
    "        :returns: bool, whether or not the two rectangles overlap.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Check if bounding boxes do not overlap (not required in solution)\n",
    "        if (\n",
    "            # (a) First bbox lower edge is GEQ second bbox upper edge\n",
    "            (min(bbox1[1], bbox2[1]) >= max(bbox1[3], bbox2[3])) or\n",
    "            # (b) First bbox right edge is LEQ second bbox left edge\n",
    "            (min(bbox1[2], bbox2[2]) <= max(bbox1[0], bbox2[0])) or\n",
    "            # (c) First bbox left edge is GEQ second bbox right edge\n",
    "            (min(bbox1[0], bbox2[0]) >= max(bbox1[2], bbox2[2])) or\n",
    "            # (d) First bbox upper edge is LEQ second bbox lower edge\n",
    "            (min(bbox1[3], bbox2[3]) <= max(bbox1[1], bbox2[1]))\n",
    "        ):\n",
    "            return False\n",
    "        # 2. Check if intersection area is larger than 0\n",
    "        else:\n",
    "            x_inter1 = max(bbox1[0], bbox2[0])\n",
    "            y_inter1 = max(bbox1[1], bbox2[1])\n",
    "            x_inter2 = min(bbox1[2], bbox2[2])\n",
    "            y_inter2 = min(bbox1[3], bbox2[3])\n",
    "            # Overlapping region must have positive area\n",
    "            w_inter = max(0, (x_inter2 - x_inter1))\n",
    "            h_inter = max(0, (y_inter2 - y_inter1))\n",
    "            if (w_inter * h_inter) > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _calculate_iou(self, gt_bbox: List[int], pred_bbox: List[int]) -> float:\n",
    "        \"\"\"Calculates the IoU score for a single pair of bounding boxes.\n",
    "\n",
    "        :param gt_bbox: 1x4 list of ground-truth coordinates,\n",
    "        :param pred_bbox: 1x4 list of predicted coordinates,\n",
    "        returns: iou, the pairwise IoU score between the two bounding boxes.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Check if bounding boxes overlap\n",
    "        if not self._overlapping_rectangles(gt_bbox, pred_bbox):\n",
    "            return 0.0\n",
    "        else:\n",
    "            # 2. Find the coordinates of the area of intersection\n",
    "            #    2a. The upper-left coordinate\n",
    "            x_inter1 = max(gt_bbox[0], pred_bbox[0])\n",
    "            y_inter1 = max(gt_bbox[1], pred_bbox[1])\n",
    "            #    2b. The lower-right coordinate\n",
    "            x_inter2 = min(gt_bbox[2], pred_bbox[2])\n",
    "            y_inter2 = min(gt_bbox[3], pred_bbox[3])\n",
    "            # 3. Calculate the area of intersection\n",
    "            # Fixed typo in solution: erroneously adding +1 to width/height\n",
    "            w_inter = max(0, x_inter2 - x_inter1)\n",
    "            h_inter = max(0, y_inter2 - y_inter1)\n",
    "            area_inter = w_inter * h_inter\n",
    "            # 4. Find the coordinates of the area of union\n",
    "            #    4a. The height and width of the first (ground truth) box\n",
    "            w_union1 = gt_bbox[2] - gt_bbox[0]        # abs() not used in sol.\n",
    "            h_union1  = gt_bbox[3] - gt_bbox[1]\n",
    "            #    4b. The height and width of the second (predicted) box\n",
    "            w_union2 = pred_bbox[2] - pred_bbox[0]    # abs() not used in sol.\n",
    "            h_union2 = pred_bbox[3] - pred_bbox[1]\n",
    "            # 5. Calculate the area of union\n",
    "            area_union = (w_union1 * h_union1) + (w_union2 * h_union2)\n",
    "            area_union -= area_inter\n",
    "            # 6. Calculate the resulting IoU score\n",
    "            iou = float(area_inter) / float(area_union)\n",
    "        return iou\n",
    "\n",
    "    def calculate_ious(self, gt_bboxes: List[List[int]], \n",
    "                       pred_bboxes: List[List[int]]) -> List[float]:\n",
    "        \"\"\"Calculates the IoU scores for all bounding box pairs.\n",
    "\n",
    "        :param gt_bboxes: Nx4 list of ground-truth coordinates,\n",
    "        :param pred_bboxes: Mx4 list of predicted coordinates,\n",
    "        returns: iou, NxM list of pairwise IoU scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Allocate a NumPy array of zeros to store the IoU scores\n",
    "        ious = np.zeros((gt_bboxes.shape[0], pred_bboxes.shape[0]))\n",
    "        # For each ground-truth bounding box\n",
    "        for i, gt_bbox in enumerate(gt_bboxes):\n",
    "            # Calculate the IoU score w.r.t the entire inference set\n",
    "            for j, pred_bbox in enumerate(pred_bboxes):\n",
    "                ious[i,j] = self._calculate_iou(gt_bbox, pred_bbox)\n",
    "        return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Simple wrapper function to get data.\"\"\"\n",
    "\n",
    "    with open('data/ground_truth.json') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    with open('data/predictions.json') as f:\n",
    "        predictions = json.load(f)\n",
    "    return ground_truth, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth, predictions = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `iou.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bboxes array\n",
    "filename = 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'\n",
    "gt_bboxes = [g['boxes'] for g in ground_truth if g['filename'] == filename][0]\n",
    "gt_bboxes = np.array(gt_bboxes)\n",
    "gt_classes = [g['classes'] for g in ground_truth if g['filename'] == filename][0]\n",
    "\n",
    "\n",
    "pred_bboxes = [p['boxes'] for p in predictions if p['filename'] == filename][0]\n",
    "# Fixing typo in solution\n",
    "pred_bboxes = np.array(pred_bboxes) # pred_boxes -> pred_bboxes\n",
    "pred_classes = [p['classes'] for p in predictions if p['filename'] == filename][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining IoU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparing results of my algorithm to provided solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84313051, 0.        , 0.        , 0.        , 0.23860974],\n",
       "       [0.        , 0.08469791, 0.4243356 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.73221757, 0.        ],\n",
       "       [0.        , 0.41277874, 0.83450504, 0.        , 0.        ],\n",
       "       [0.        , 0.68758782, 0.43810509, 0.        , 0.        ],\n",
       "       [0.12221933, 0.        , 0.        , 0.        , 0.66359447],\n",
       "       [0.02888778, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.02499868, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing my IoU algorithm\n",
    "ious = IoU().calculate_ious(gt_bboxes, pred_bboxes) # pred_boxes -> pred_bboxes\n",
    "ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84313051, 0.        , 0.        , 0.        , 0.23860974],\n",
       "       [0.        , 0.08469791, 0.4243356 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.73221757, 0.        ],\n",
       "       [0.        , 0.41277874, 0.83450504, 0.        , 0.        ],\n",
       "       [0.        , 0.68758782, 0.43810509, 0.        , 0.        ],\n",
       "       [0.12221933, 0.        , 0.        , 0.        , 0.66359447],\n",
       "       [0.02888778, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.02499868, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing Udacity's IoU algorithm\n",
    "ious_sol = IoUSolution().calculate_ious(gt_bboxes, pred_bboxes)\n",
    "ious_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating IoU score results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(ious):\n",
    "    \"\"\"Checks the IoU prediction results.\"\"\"\n",
    "\n",
    "    solution = np.load('data/exercise1_check.npy')\n",
    "    print((ious == solution).sum())\n",
    "    assert (ious == solution).sum() == 40, 'The iou calculation is wrong!'\n",
    "    print('Congrats, the iou calculation is correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Congrats, the iou calculation is correct!\n"
     ]
    }
   ],
   "source": [
    "check_results(ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJcSKX0jHzAq"
   },
   "source": [
    "### 2.2. Precision and Recall for Bounding Box Prediction\n",
    "In order to evaluate the performance of our Intersection over Union (IoU) function, we will look at two very popular classification metrics in Machine Learning: [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). In a binary classification task, precision and recall are measures of relevance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVFJlitsNxap"
   },
   "source": [
    "_Precision_ is the ratio of correct predictions to total number of predictions made, i.e., the true positives. In an information retrieval context, _precision_ addresses the question: _How many retrieved items are relevant?_.\n",
    "* $Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "In a binary classification task, a perfect precision score of `1.0` means that every predicted label of a respective class was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0avsprWN5Tx"
   },
   "source": [
    "_Recall_ is the ratio of correct predictions to total number of labels in the positive class. In an information retrieval context, _recall_ adresses the question: _How many relevant items are retrieved?_\n",
    "* $Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "In a binary classification task, a perfect recall score of `1.0` means that every item from a respective class was predicted correctly, i.e., labelled as belonging to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for our problem\n",
    "_Precision_ and _recall_ are often defined for binary classification problems involving a set of \"correct\" and \"incorrect\" class label predictions. In such problems, the true positive (_TP_) and true negative (_TN_) values are simply the number of _correct_ positive and negative class label predictions. It follows that the false positive (_FP_) and false negative (_FN_) values are the number of _incorrect_ class label predictions.\n",
    "\n",
    "In the context of object recognition, we must adapt these definitions to fit the pairwise continuous IoU scores we computed earlier. In order to count the number of _correct_ and _incorrect_ bounding box predictions, we will use a _threshold value_ of $0.5$. That is, any IoU score above the threshold value of $0.5$ will be considered _correct_, and any IoU score below the threshold value will be considered _incorrect_. \n",
    "\n",
    "One other important consideration specific to the bounding box prediction task is the _elimination of the TN term_. A non-zero true negative (TN) value would indicate that our model has sometimes predicted \"no bounding box\" in an image that does in fact _not_ contain a bounding box. Since in our ground truth dataset we are assured that every sample contains a single bounding box, we know that a non-zero TN value could not be possible and that \"no bounding box\" is not a prediction our model makes. In summary, every sample in `pred_bboxes` corresponds to a predicted bounding box based on its ground truth sample which is gauranteed to contain a bounding box somewhere in the image frame.\n",
    "* **True Positives (TP)**: set of bounding box predictions an IoU score greater than the threshold value (0.5) where the ground truth and predicted class labels are the same;\n",
    "    * `the image contains a bounding box of class i and model predicts bounding box for class i with IoU > 0.5`.\n",
    "* **False Positives (FP)**: set of bounding box predictions with an IoU score greater than the threshold value (0.5) where the predicted and ground truth class labels are _NOT_ the same;\n",
    "    * `the image contains a bounding box of class i and model predicts bounding box for class j with IoU > 0.5.`\n",
    "* **True Negative (TN):** set of bounding box pairs whose ground truth and predicted bounding boxes are 'None';\n",
    "    * `not applicable in our dataset.`\n",
    "* **False Negative (FN):** set of ground truth bounding boxes where the corresponding class predictions have IoU scores lower than the threshold.\n",
    "    * `the image contains a bounding box of class i and model predicts no bounding box (IoU = 0.0) or bounding box for class i with low confidence (IoU < 0.5).`\n",
    "\n",
    "Based on the extended definition above, we can now safely determine the _precision_ and _recall_ scores for our entire set of pairwise IoUs.\n",
    "\n",
    "\n",
    "#### Classes in our dataset\n",
    "Note that in our bounding box dataset, we have two class labels `1` and `2`. We are provided a list of ground truth class labels, `gt_classes`, and a list of predicted class labels, `pred_classes`, as follows\n",
    "```\n",
    "gt_classes = [1, 1, 1, 1, 2, 1, 1, 1]\n",
    "pred_classes = [1, 2, 1, 2, 1]\n",
    "\n",
    "```\n",
    "In our IoU scoring algorithm, we computed an `iou` score for each ground truth bounding box against the set of predicted boundng boxes in a row-column wise matrix, i.e., each `[row, col]` pair in our `ious` matrix corresponds to a ground truth-prediction bounding box pair. We can visualise this a bit easier below when turning the 2D matrix into a [pandas `DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) object.\n",
    "\n",
    "#### Revisiting the example in Fig. 4\n",
    "\n",
    "In Fig. 4 we have a single pair of bounding boxes with a resulting IoU score of $0.72$. Since this confidence value is above our threshold of $0.5$, we will count the bounding box prediction in our set of true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qotRS8TJN8lV"
   },
   "source": [
    "#### Precision and recall algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "udZUn6LZDxj1"
   },
   "outputs": [],
   "source": [
    "### From Udacity's `precision_recall.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "sdg-kCvBD8F5"
   },
   "outputs": [],
   "source": [
    "class PrecisionRecall:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def _compute_class_metrics(self, ious: List[float], gt_classes: List[int],\n",
    "                               pred_classes: List[int], iou_threshold: float) -> List[dict]:\n",
    "        \"\"\"Computes the classification metrics for a multi-class dataset.\n",
    "        \n",
    "        Dataset contains pairwse IoU scores for the bounding box prediction problem.\n",
    "        \n",
    "        :param ious: NxM list of pairwise IoU scores\n",
    "        :param gt_classes: 1xN list of ground truth class labels\n",
    "        :param pred_classes: 1xM list of predicted class labels\n",
    "        :param iou_threshold: float threshold, predictions 'valid' above this value.\n",
    "        :returns: list of dict objects containing the per-class metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store per-class metrics\n",
    "        cls_metrics = []\n",
    "        # Convert matrix into pandas DataFrame for easier slicing/indexing\n",
    "        df = pd.DataFrame(data=ious, index=gt_classes, columns=pred_classes)\n",
    "        # Compute per-class metrics\n",
    "        for cls in np.unique(gt_classes):\n",
    "            # Get all preds for gt cls label\n",
    "            cls_df = df.loc[[cls],:]    # Preserves DataFrame structure\n",
    "            print(cls_df)\n",
    "            # Get all pairs with matching class labels\n",
    "            cls_data = cls_df[cls]\n",
    "            print(cls_data)\n",
    "            # Count number of TN (i.e., no bounding box to predict, N/A for our data)\n",
    "            true_negatives = 0\n",
    "            # Count number of TP (correct class, IoU > 0.5) predictions\n",
    "            true_positives = np.count_nonzero(cls_data.where(cls_data > 0.5).fillna(0))\n",
    "            # Count number of FN (incorrect class, IoU < 0.5) predictions\n",
    "            false_negatives = np.count_nonzero(cls_df.where(cls_df < 0.5).drop(columns=cls).fillna(0))\n",
    "            # Count number of FP (incorrect class, IoU > 0.5) predictions\n",
    "            false_positives = np.count_nonzero(cls_df.where(cls_df > 0.5).drop(columns=cls).fillna(0))\n",
    "            # Calculate precsion/recall for each class and store in dict\n",
    "            cls_dict = {'TN': true_negatives,\n",
    "                        'TP': true_positives,\n",
    "                        'FN': false_negatives,\n",
    "                        'FP': false_positives\n",
    "                        }\n",
    "            cls_metrics.append(cls_dict)\n",
    "        return cls_metrics\n",
    "\n",
    "    def precision_recall(self, ious: List[float], gt_classes: List[int], \n",
    "                         pred_classes: List[int], iou_threshold:float=0.5) -> (float, float):\n",
    "        \"\"\"Calculates the precision and recall metrics.\n",
    "        \n",
    "        Dataset contains pairwse IoU scores for the bounding box prediction problem.\n",
    "        Columns are the predicted class labels, rows are the ground truth class labels.\n",
    "        \n",
    "        :param ious: NxM list of pairwise IoU scores\n",
    "        :param gt_classes: 1xN list of ground truth class labels\n",
    "        :param pred_classes: 1xM list of predicted class labels\n",
    "        :param iou_threshold: float threshold, predictions 'valid' above this value.\n",
    "        :returns: (precision, recall), the two classification metric values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute per-class metrics\n",
    "        cls_metrics = self._compute_class_metrics(ious, gt_classes, pred_classes, iou_threshold)\n",
    "        # Store total metrics\n",
    "        TN_total, TP_total = 0, 0\n",
    "        FN_total, FP_total = 0, 0\n",
    "        for c in cls_metrics:\n",
    "            TN_total += c['TN']\n",
    "            TP_total += c['TP']\n",
    "            FN_total += c['FN']\n",
    "            FP_total += c['FP']\n",
    "        # Print total metrics\n",
    "        print(\"TP:\", TP_total, \"TN:\", TN_total, \"\\nFP:\", FP_total, \"FN:\", FN_total)\n",
    "        # Compute combined metrics\n",
    "        precision = TP_total / float(TP_total + FP_total)\n",
    "        recall = TP_total / float(TP_total + FN_total)\n",
    "        return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `solution/precision_recall.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionRecallSolution:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def _compute_metrics(self, ious: List[float], gt_classes: List[int], \n",
    "                         pred_classes: List[int], iou_threshold: float) -> List[dict]:\n",
    "        \"\"\"Computes the dataset-specific classification metrics.\n",
    "        \n",
    "        Dataset contains pairwse IoU scores for the bounding box prediction problem.\n",
    "        \n",
    "        :param ious: NxM list of pairwise IoU scores\n",
    "        :param gt_classes: 1xN list of ground truth class labels\n",
    "        :param pred_classes: 1xM list of predicted class labels\n",
    "        :param iou_threshold: float threshold, predictions 'valid' above this value.\n",
    "        :returns: list of dict objects containing the per-class metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store total metrics\n",
    "        metrics = []\n",
    "        # Store true positives/false positives\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        # Get all IoU values above threshold\n",
    "        xs, ys = np.where(ious > iou_threshold)\n",
    "        print(xs, ys)\n",
    "        # Go through all (row, col) matrix elements above threshold\n",
    "        for x, y in zip(xs, ys):\n",
    "            # Get corresponding class labels for each pairwise IoU\n",
    "            if gt_classes[x] == pred_classes[y]:\n",
    "                # If class labels match and above IoU threshold\n",
    "                TP += 1\n",
    "            else:\n",
    "                # If class labels do not match and above IoU threshold\n",
    "                FP += 1\n",
    "        # Get number of bbox pairs above IoU threshold with matching labels\n",
    "        matched_gt = len(np.unique(xs)) # use unique to avoid one ground truth matched to multiple preds.\n",
    "        # Get number of bbox pairs wth mismatched labels / IoU below threshold\n",
    "        FN = len(gt_classes) - matched_gt\n",
    "        # Get number of gt samples with no bounding box\n",
    "        TN = 0\n",
    "        # Store results in dict, return dict in list\n",
    "        return [{'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}]\n",
    "\n",
    "    def precision_recall(self, ious: List[float], gt_classes: List[int],\n",
    "                         pred_classes: List[int], iou_threshold:float=0.5) -> (float, float):\n",
    "        \"\"\"Calculates the precision and recall metrics.\n",
    "        \n",
    "        Dataset contains pairwse IoU scores for the bounding box prediction problem.\n",
    "        Columns are the predicted class labels, rows are the ground truth class labels.\n",
    "        \n",
    "        :param ious: NxM list of pairwise IoU scores\n",
    "        :param gt_classes: 1xN list of ground truth class labels\n",
    "        :param pred_classes: 1xM list of predicted class labels\n",
    "        :param iou_threshold: float threshold, predictions 'valid' above this value.\n",
    "        :returns: (precision, recall), the two classification metric values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute metrics\n",
    "        cls_metrics = self._compute_metrics(ious, gt_classes, pred_classes, iou_threshold)\n",
    "        # Store total metrics\n",
    "        TN_total, TP_total = 0, 0\n",
    "        FN_total, FP_total = 0, 0\n",
    "        for c in cls_metrics:\n",
    "            TN_total += c['TN']\n",
    "            TP_total += c['TP']\n",
    "            FN_total += c['FN']\n",
    "            FP_total += c['FP']\n",
    "        # Print total metrics\n",
    "        print(\"TP:\", TP_total, \"TN:\", TN_total, \"\\nFP:\", FP_total, \"FN:\", FN_total)\n",
    "        # Compute combined metrics\n",
    "        precision = TP_total / float(TP_total + FP_total)\n",
    "        recall = TP_total / float(TP_total + FN_total)\n",
    "        return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining precision and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualising our pairwise IoU scores in a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.843131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084698</td>\n",
       "      <td>0.424336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732218</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412779</td>\n",
       "      <td>0.834505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687588</td>\n",
       "      <td>0.438105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         1         2         1\n",
       "1  0.843131  0.000000  0.000000  0.000000  0.238610\n",
       "1  0.000000  0.084698  0.424336  0.000000  0.000000\n",
       "1  0.000000  0.000000  0.000000  0.732218  0.000000\n",
       "1  0.000000  0.412779  0.834505  0.000000  0.000000\n",
       "2  0.000000  0.687588  0.438105  0.000000  0.000000\n",
       "1  0.122219  0.000000  0.000000  0.000000  0.663594\n",
       "1  0.028888  0.000000  0.000000  0.000000  0.000000\n",
       "1  0.000000  0.000000  0.024999  0.000000  0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=ious, index=gt_classes, columns=pred_classes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1         2         1         2         1\n",
      "1  0.843131  0.000000  0.000000  0.000000  0.238610\n",
      "1  0.000000  0.084698  0.424336  0.000000  0.000000\n",
      "1  0.000000  0.000000  0.000000  0.732218  0.000000\n",
      "1  0.000000  0.412779  0.834505  0.000000  0.000000\n",
      "1  0.122219  0.000000  0.000000  0.000000  0.663594\n",
      "1  0.028888  0.000000  0.000000  0.000000  0.000000\n",
      "1  0.000000  0.000000  0.024999  0.000000  0.000000\n",
      "          1         1         1\n",
      "1  0.843131  0.000000  0.238610\n",
      "1  0.000000  0.424336  0.000000\n",
      "1  0.000000  0.000000  0.000000\n",
      "1  0.000000  0.834505  0.000000\n",
      "1  0.122219  0.000000  0.663594\n",
      "1  0.028888  0.000000  0.000000\n",
      "1  0.000000  0.024999  0.000000\n",
      "     1         2         1    2    1\n",
      "2  0.0  0.687588  0.438105  0.0  0.0\n",
      "          2    2\n",
      "2  0.687588  0.0\n",
      "TP: 4 TN: 0 \n",
      "FP: 1 FN: 3\n"
     ]
    }
   ],
   "source": [
    "### Testing my Precision/Recall algorithm\n",
    "precision, recall = PrecisionRecall().precision_recall(ious, gt_classes, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8 \n",
      "Recall: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision, \"\\nRecall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 4 5] [0 3 2 1 4]\n",
      "TP: 4 TN: 0 \n",
      "FP: 1 FN: 3\n"
     ]
    }
   ],
   "source": [
    "### Testing the Udacity Precision/Recall solution\n",
    "precision_sol, recall_sol = PrecisionRecallSolution().precision_recall(ious_sol, gt_classes, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8 \n",
      "Recall: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision_sol, \"\\nRecall:\", recall_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this comparison we see that my `PrecisionRecall` algorithm correctly computed the precision and recall metrics according to the solution provided by Udacity. While Udacity's algorithm is simplistic, my algorithm is both readable and able to scale for multi-class datasets. This will be a useful approach when calculating average precision for the $mAP$ metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Mean Average Precision (mAP) for Bounding Box Prediction (optional)\n",
    "Since we are given a dataset containing multiple class labels, i.e., bounding boxes belonging to a respective object type. While calculating mAP is not a requirement in this exercise, it is generally a good metric to observe in practice. The mAP metric can help us understand how well each classes' bounding boxes are being predicted. If you are up for the challenge, I'd encourage you to try completing this task!\n",
    "\n",
    "#### Considerations for our problem\n",
    "We are given a set of ground truth values which correspond to the true coordinates of each bounding box in our dataset. We made full use of this data in order to score the prediction set and obtain precision/recall values. We are also given a class label for each _ground truth_ and _predicted_ bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0PGLoU4oEhkT"
   },
   "outputs": [],
   "source": [
    "### Exercise left to the reader, will be revisited by author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Closing Remarks\n",
    "\n",
    "##### Alternatives to IoU\n",
    "* [Generalized IoU (GIoU)](https://giou.stanford.edu/): loss function that gives weight to bounding box predictions that are \"closer\" to ground truth despite having zero overlap, e.g., when the standard $IoU=0.0$;\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    GIoU &=  \\frac{\\left \\vert A \\cap B \\right \\vert}{\\left \\vert A \\cup B \n",
    "    \\right \\vert} - \\frac{\\left \\vert C \\setminus (A \\cup B) \\right \\vert}{\\left \\vert C \\right \\vert}. \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "* [Mean Average Precision (mAP)](https://towardsdatascience.com/what-is-average-precision-in-object-detection-localization-algorithms-and-how-to-calculate-it-3f330efe697b): alternative to IoU; used in popular object detection models like YOLO and Faster RCNN; takes the average precision (AP) across all classes;\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    mAP &=  \\frac{1}{N} * \\sum_{i=1}^{N} AP_{i}. \\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "##### Other purposes for IoU\n",
    "IoU is a popular metric in object detection for bounding box prediction tasks. IoU serves an important role in _Non-Maxima Supression_, which is an algorithm to retain a single bounding box from a set of predictions with the greatest amount of overlap (in order to discard the rest). IoU can also be used as a loss metric or cost function in many deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Future Work\n",
    "- ✅ Visualise ground truth versus prediction bounding boxes (see [Exercise 1.1.2](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-2-Data-Acquisition-Visualisation/2022-08-01-Data-Acquisition-Visualisation.ipynb));\n",
    "- ✅ Clean up precision/recall formulas, considering e.g., association matrices (see [Exercise 1.5.2](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-5-2-Mean-Average-Precision/2022-09-25-Mean-Average-Precision.ipynb));\n",
    "- ✅ Compute per-class statistics efficiently, implementing e.g., mAP (see [Exercise 1.5.2](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-5-2-Mean-Average-Precision/2022-09-25-Mean-Average-Precision.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "731T8ym4T16t"
   },
   "source": [
    "## Credits\n",
    "This assignment was prepared by Thomas Hossler and Michael Virgo et al., Winter 2021 (link [here](https://github.com/udacity/nd013-c1-vision-starter)).\n",
    "\n",
    "References:\n",
    "* [1] _Generalized Intersection over Union_, Rezatofighi et al., The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. https://giou.stanford.edu.\n",
    "\n",
    "Helpful resources:\n",
    "* [IOU (Intersection over Union) by V.S. Subramanyam](https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef)\n",
    "\n",
    "* [Intersection over Union by A. Persson | YouTube](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/metrics/iou.py)\n",
    "\n",
    "* [Rectangle Intersection Demonstration by M. Crumley](https://silentmatt.com/rectangle-intersection/)\n",
    "\n",
    "* [Determine if two rectangles overlap each other? | Stack Overflow](https://stackoverflow.com/questions/306316/determine-if-two-rectangles-overlap-each-other)\n",
    "\n",
    "* [Problem 836. Rectangle Overlap | LeetCode](https://leetcode.com/problems/rectangle-overlap/)\n",
    "\n",
    "* [Practitioner's guide to IoU, Non-Max Supression, and Mean Average Precision | Medium article](https://vijayabhaskar96.medium.com/practitioners-guide-to-iou-non-max-suppression-and-mean-average-precision-e09de73a2bd8)\n",
    "\n",
    "* [What is Average Precision in Object Detection? | Medium article](https://towardsdatascience.com/what-is-average-precision-in-object-detection-localization-algorithms-and-how-to-calculate-it-3f330efe697b)\n",
    "\n",
    "* [One value for recall and precision for multiple classes | Udacity Knowledge forum](https://knowledge.udacity.com/questions/857563)\n",
    "\n",
    "* [Keep selected column as DataFrame instead of Series | Stack Overflow](https://stackoverflow.com/a/56983052/8222897)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2022-07-25-Choosing-Metrics-IoU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59594960",
   "metadata": {},
   "source": [
    "# Exercise 1.5.3 - Learning Rate Schedules\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabeb51",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad1da3",
   "metadata": {},
   "source": [
    "* Implement two [learning rate schedules](https://en.wikipedia.org/wiki/Learning_rate#Learning_rate_schedule): the [exponential decay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) and [step-wise annealing](https://paperswithcode.com/method/step-decay) strategies;\n",
    "* Use the off-the-shelf [`tf.keras.optimizers.schedules`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) and the custom LR schedule [`tf.keras.callbacks.LearningRateScheduler`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) wrapper to implement the above strategies;\n",
    "* Evaluate a lightweight deep neural network (simple [ConvNet](https://en.wikipedia.org/wiki/Convolutional_neural_network)) with the LR schedules on the [GTSRB](https://benchmark.ini.rub.de/gtsrb_dataset.html) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe88f3",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83af251",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666c2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f30907",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8891c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad9649",
   "metadata": {},
   "source": [
    "### 1.1. Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d22fe1",
   "metadata": {},
   "source": [
    "In machine learning and statistics, the [learning rate](https://en.wikipedia.org/wiki/Learning_rate) is a tunable hyperparameter in an optimisation algorithm that determines the step size at each iteration while moving towards a minimum of a loss function (credit: Wikipedia). Setting learning rates optimally is often a balancing act between over- and overshooting a global minima. When the learning rate is _too low_, a model might fail to converge as its steps in the direction of a function minima are simply too small. On the other hand, a learning rate that is _too high_ might result in extremely large steps that _overshoot_ the function minima and miss the target entirely. An optimal (fixed) learning rate value should be selected such that the likelihood of overshooting is minimised and that is sufficiently large to perform steepest descent towards convergence.\n",
    "\n",
    "A [learning rate schedule](https://en.wikipedia.org/wiki/Learning_rate#Learning_rate_schedule) helps accomplish this by not only decreasing overshoot but also speeding up the time it could take to reach convergence. LR schedules have two important properties: _decay_ and _momentum_. Decay is a hyperparameter that controls overshooting by decreasing (_annealing_) the learning rate by a fixed factor. Momentum is another hyperparameter that helps us speed up the convergence time. Analogous to a ball rolling down a hill; momentum governs how quickly our learning rate decays. This is extremely useful for making sure that the direction we move in towards steepest descent is indeed towards the global minima and not just towards a local minimum (as a ball with little to no momentum will struggle to 'get over' the shallow [saddle points](https://en.wikipedia.org/wiki/Saddle_point) of the differentiable function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba526ddc",
   "metadata": {},
   "source": [
    "#### Step-wise Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394771e",
   "metadata": {},
   "source": [
    "One of the easiest learning rate schedules to implement is the [step-wise method](https://paperswithcode.com/method/step-decay). With this schedule, the learning rate is decreased by a static factor at evenly-spaced intervals during the training cycle (usually measured in epochs, i.e., full passes over a dataset). The scale factor $\\gamma$ serves as a hyperparameter governing how much to decrease the previous learning rate $\\eta_{i-1}$ by,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta_{i} &= \\eta_{i-1} * \\gamma, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for the updated learning rate value $\\eta_{n}$.\n",
    "\n",
    "Step-wise annealing can also be, well, _step-wise_. As the name implies, the number of current _steps_ $n$ can also be used to scale the initial fixed learning rate value $\\eta_{0}$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta_{i} = \\eta_{0} * d^{\\left\\lfloor\\frac{1 + n}{r}\\right\\rfloor},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "given a static decay factor $d$ (e.g., $d=0.5$ will decay the LR by half) and a drop-rate $r$ (e.g., $r=10$ will drop the LR every 10 iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a9977",
   "metadata": {},
   "source": [
    "#### Exponential Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6c596",
   "metadata": {},
   "source": [
    "[Exponential Decay](https://paperswithcode.com/method/exponential-decay) is similar to step-wise annealing in that the LR is gradually decreased over time. However, instead of a linear step-based decay, a scaled, reflected exponential function is used to reduce the initial fixed learning rate $\\eta_{0}$ over time,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta_{n} &= \\eta_{0} * \\mathcal{e}^{-dn}, \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "given a static decay factor $d$ and the current number of iterations $n$. Depending on the choice of the decay factor $d$, the Exponential Decay schedule could (preferably) accelerate the learning rate decay more rapidly than with step-wise annealing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3cd85",
   "metadata": {},
   "source": [
    "### 1.2. Adaptive Learning Rate Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb72a30",
   "metadata": {},
   "source": [
    "Given that both the learning rate and its schedules have hyperparameters that need to be manually selected and defined prior to training, a learning rate schedule might not always be optimal. Instead, we can use [adaptive learning rate](https://en.wikipedia.org/wiki/Learning_rate#Adaptive_learning_rate) methods to utilise heuristic approaches to parameter selection that provide reliable results. Adaptive learning rate methods such as [Adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) build upon the [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) optimiser with adaptive learning rate tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36656f",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a3aed",
   "metadata": {},
   "source": [
    "The [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad) (adaptive gradient algorithm) [1] was pioneered in 2011 by J. Duchi et al., and is a dynamic technique for the normalisation of parameter updates. In brief; weights who receive large gradient update values will have their effective learning rate reduced. Conversely, weights with small or infrequently-updated values will have their effective learning rate increased. Using a parameter referred to as a \"cache\" variable $c$, AdaGrad performs the following updates,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c &= c + dx^2, \\\\\n",
    "x &= x - \\frac{\\alpha * dx}{\\sqrt{x} + \\epsilon}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The cache variable $c$ keeps track of the per-parameter sum of the squared gradients, which in turn is used to normalise the parameter updates. Here an $\\epsilon$ term is added to the denominator of the update term for numerical stability (to avoid a divide by zero error), credit: [G. Singh](https://medium.com/@gsinghviews/adaptive-learning-rate-methods-e6e00dcbae5e)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45166b34",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6bf43b",
   "metadata": {},
   "source": [
    "_Coming soon_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10960d26",
   "metadata": {},
   "source": [
    "#### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4427b5",
   "metadata": {},
   "source": [
    "_Coming soon_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28665a2f",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef87e1c",
   "metadata": {},
   "source": [
    "To do so, you will have to leverage Keras `callbacks`. Callbacks performs various action\n",
    "at different stages of training. For example, Keras uses a callback to save the models weights at \n",
    "the end of each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae24019",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        history = self.model.history.history\n",
    "        history['lr'] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        history = self.model.history.history\n",
    "        optimizer = self.model.optimizer\n",
    "        decayed_lr = optimizer._decayed_lr('float32').numpy()\n",
    "        history['lr'].append(decayed_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f923b3",
   "metadata": {},
   "source": [
    "You can either use pre-implemented schedulers (see Tips) or implement a scheduler yourself \n",
    "using your own custom decay function, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f1a83",
   "metadata": {},
   "source": [
    "```\n",
    "def decay(model, callbacks, lr=0.001):\n",
    "    \"\"\" create custom decay that does not do anything \"\"\"\n",
    "    def scheduler(epoch, lr):\n",
    "        return lr \n",
    "\n",
    "    callbacks.append(tf.keras.callbacks.LearningRateScheduler(scheduler))\n",
    "\n",
    "    # compile model\n",
    "    model.compile()\n",
    "    \n",
    "    return model, callbacks \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbeedbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9420ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(\n",
    "        model: tf.keras.Model, callbacks: List[tf.keras.callbacks.Callback]=[], \n",
    "        initial_lr: float=0.001\n",
    ") -> Tuple[tf.keras.Model, List[tf.keras.callbacks.Callback]]:\n",
    "    \"\"\"Compiles and returns Model instance with exponential decay LR schedule.\n",
    "    \n",
    "    :param model: the tf.keras.Model instance to compile.\n",
    "    :param callbacks: the list of tf.keras.callbacks to pass alongside model.\n",
    "    :param initial_lr: the value to fix the learning rate at before annealing.\n",
    "    :returns: tuple, the compiled Model instance and its callbacks.\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    \n",
    "    # Instantiate the learning rate schedule\n",
    "    lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                        initial_learning_rate=initial_lr,\n",
    "                        decay_steps=100,\n",
    "                        decay_rate=0.95,\n",
    "                        staircase=False\n",
    "    )\n",
    "    # Instantiate the optimiser\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy']\n",
    "    )\n",
    "    # Return the model and any specified callbacks\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89588401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:19:24.263631: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfd375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = exponential_decay(model=model, callbacks=[], initial_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71555f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(\n",
    "        model: tf.keras.Model, callbacks: List[tf.keras.callbacks.Callback]=[], \n",
    "        initial_lr: float=0.001\n",
    ") -> Tuple[tf.keras.Model, List[tf.keras.callbacks.Callback]]:\n",
    "    \"\"\"Compiles and returns Model instance with step-wise decay LR schedule.\n",
    "    \n",
    "    :param model: the tf.keras.Model instance to compile.\n",
    "    :param callbacks: the list of tf.keras.callbacks to pass alongside model.\n",
    "    :param initial_lr: the value to fix the learning rate at before annealing.\n",
    "    :returns: tuple, the compiled Model instance and its callbacks.\n",
    "    \"\"\"\n",
    "    #  IMPLEMENT THIS FUNCTION\n",
    "    \n",
    "    def scheduler(epoch: int, lr: float):\n",
    "        \"\"\"Simple custom constant step-wise annealing schedule.\"\"\"\n",
    "        return lr / 2 if epoch % 10 == 0 and epoch > 0 else lr\n",
    "    \n",
    "    # Instantiate a custom Keras callback to perform LR annealing\n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "                            schedule=scheduler, verbose=1\n",
    "    )\n",
    "    callbacks.append(lr_schedule)\n",
    "    # Instantiate the optimiser with the initial learning rate value\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy']\n",
    "    )\n",
    "    # Return the compiled model and the custom LR scheduler and any other callback\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d372ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678de8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = step_decay(model=model, callbacks=[], initial_lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c6847",
   "metadata": {},
   "source": [
    "Feel free to use any decay rates as well as a step size of your choice for the stepwise scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb8898",
   "metadata": {},
   "source": [
    "You can run `python training.py` to see the effect of different annealing strategies on your training and model performances. Make sure to feed in the GTSRB dataset as the image directory, and use the Desktop to view the visualization of final training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_logger(mod_name):\n",
    "    logger = logging.getLogger(mod_name)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e831de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_module_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6772c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Download and process tf files')\n",
    "parser.add_argument('-d', '--imdir', required=True, type=str,\n",
    "                    help='data directory')\n",
    "parser.add_argument('-e', '--epochs', default=10, type=int,\n",
    "                    help='Number of epochs')\n",
    "args = parser.parse_args()    \n",
    "\n",
    "logger.info(f'Training for {args.epochs} epochs using {args.imdir} data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b93e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image,label):\n",
    "    \"\"\" small function to normalize input images \"\"\"\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b576ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(imdir):\n",
    "    \"\"\" extract GTSRB dataset from directory \"\"\"\n",
    "    train_dataset = image_dataset_from_directory(imdir, \n",
    "                                       image_size=(32, 32),\n",
    "                                       batch_size=32,\n",
    "                                       validation_split=0.2,\n",
    "                                       subset='training',\n",
    "                                       seed=123,\n",
    "                                       label_mode='int')\n",
    "\n",
    "    val_dataset = image_dataset_from_directory(imdir, \n",
    "                                        image_size=(32, 32),\n",
    "                                        batch_size=32,\n",
    "                                        validation_split=0.2,\n",
    "                                        subset='validation',\n",
    "                                        seed=123,\n",
    "                                        label_mode='int')\n",
    "    train_dataset = train_dataset.map(process)\n",
    "    val_dataset = val_dataset.map(process)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edd07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the datasets\n",
    "train_dataset, val_dataset = get_datasets(args.imdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88eb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = LrLogger()\n",
    "callbacks = [logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d22d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeb930b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8148b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(\n",
    "        inputs: tf.keras.Input, outputs: tf.keras.layers.Layer\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"Creates a tf.keras.Sequential Model with the provided inputs and outputs.\n",
    "    \n",
    "    :param inputs: the tf.keras.Input layer of specified shape.\n",
    "    :param outputs: the tf.keras.layers.Layer instance of desired output,\n",
    "        this should be a Dense layer with units equal to num. classes.\n",
    "    :returns: the tf.keras.Model instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    net = tf.keras.models.Sequential([\n",
    "        inputs,\n",
    "        tf.keras.layers.Conv2D(\n",
    "                filters=6, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=(2, 2), strides=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "                filters=16, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=(2, 2), strides=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "                units=120, activation='relu'),\n",
    "        tf.keras.layers.Dense(\n",
    "                units=84, activation='relu'),\n",
    "        outputs\n",
    "    ])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baf04944",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "070f810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (32, 32, 3)\n",
    "N_CLASSES = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb807961",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=INPUT_SHAPE)\n",
    "outputs = tf.keras.layers.Dense(units=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84a677b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d95251e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 6)         168       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                3655      \n",
      "=================================================================\n",
      "Total params: 84,107\n",
      "Trainable params: 84,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a list of all desired TensorFlow Keras callbacks\n",
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile the model with a Keras `ExponentialDecay` LR schedule\n",
    "model, callbacks = step_decay(model, callbacks=callbacks, initial_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile the model with a custom `step_decay` LR schedule\n",
    "# model = create_network(inputs, outputs)\n",
    "# model, callbacks = step_decay(model, callbacks=callbacks, initial_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the model on the train data and passing in our callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_dataset, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(history):\n",
    "    \"\"\" plot loss and accuracy from keras history object \"\"\"\n",
    "    f, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].plot(history.history['loss'], linewidth=3)\n",
    "    ax[0].plot(history.history['val_loss'], linewidth=3)\n",
    "    ax[0].set_title('Loss', fontsize=16)\n",
    "    ax[0].set_ylabel('Loss', fontsize=16)\n",
    "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[0].legend(['train loss', 'val loss'], loc='upper right')\n",
    "    ax[1].plot(history.history['accuracy'], linewidth=3)\n",
    "    ax[1].plot(history.history['val_accuracy'], linewidth=3)\n",
    "    ax[1].set_title('Accuracy', fontsize=16)\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
    "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[1].legend(['train acc', 'val acc'], loc='upper left')\n",
    "    ax[2].plot(history.history['lr'], linewidth=3)\n",
    "    ax[2].set_title('Learning rate', fontsize=16)\n",
    "    ax[2].set_ylabel('Learning Rate', fontsize=16)\n",
    "    ax[2].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[2].legend(['learning rate'], loc='upper right')\n",
    "    # ax[2].ticklabel_format(axis='y', style='sci')\n",
    "    ax[2].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52de52",
   "metadata": {},
   "source": [
    "## 3. Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8c166",
   "metadata": {},
   "source": [
    "##### Alternatives\n",
    "* Test out various starting learning rate values.\n",
    "\n",
    "##### Extensions to task\n",
    "* Test learning rate strategies on other model architectures;\n",
    "* Visualise the learning rate schedule over time;\n",
    "* Implement other [popular](https://paperswithcode.com/methods/category/learning-rate-schedules) learning rate schedules in literature (e.g., [cosine annealing](https://paperswithcode.com/method/cosine-annealing), [linear warmup with linear decay](https://paperswithcode.com/method/linear-warmup-with-linear-decay));\n",
    "* Implement [adaptive learning rate](https://en.wikipedia.org/wiki/Learning_rate#Adaptive_learning_rate) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5b974",
   "metadata": {},
   "source": [
    "## 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b6c62",
   "metadata": {},
   "source": [
    "- [ ] Visualise learning rate schedule over time;\n",
    "- [ ] Compare their performance / affect on model accuracy;\n",
    "- [ ] Implement other popular learning rate schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9168c",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2e2c1",
   "metadata": {},
   "source": [
    "This assignment was prepared by Thomas Hossler et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "\n",
    "References\n",
    "* [1] Duchi, J. et al., Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Researcher. 12(61):2121â€“2159. [doi:10.5555/1953048.2021068](https://dl.acm.org/doi/10.5555/1953048.2021068).\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [Learning Rate Schedule in Practice: an example with Keras and TensorFlow 2.0 by B. Chen | Medium](https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c)\n",
    "* [Adaptive Learning Rate Methods by G. Singh](https://medium.com/@gsinghviews/adaptive-learning-rate-methods-e6e00dcbae5e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

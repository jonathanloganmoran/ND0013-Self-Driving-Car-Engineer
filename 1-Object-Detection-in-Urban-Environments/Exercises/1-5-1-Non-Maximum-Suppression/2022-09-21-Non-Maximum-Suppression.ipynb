{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e84f98",
   "metadata": {},
   "source": [
    "# Exercise 1.5.1 - Non-Maximum Suppression\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered by Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0be6f8",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a42b72",
   "metadata": {},
   "source": [
    "* Implement the Non-Maximum Suppression ([NMS](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)) algorithm;\n",
    "* Use the Intersection over Union ([IoU](https://en.wikipedia.org/wiki/Jaccard_index)) metric with a threshold value of $0.7$; \n",
    "* Apply the NMS algorithm to the provided frame from the [Waymo Open Dataset](https://waymo.com/open/);\n",
    "* (Optional) Use the [Soft-NMS](https://arxiv.org/abs/1704.04503) algorithm to re-score the bounding box predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c09c9",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed0c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b949b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors, patches\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b34082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25331dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 14:45:01.765625: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffae077",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8831a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43195676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489b59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cb00b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c516ae",
   "metadata": {},
   "source": [
    "### 1.1. Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bcf76",
   "metadata": {},
   "source": [
    "#### Background\n",
    "* Why is it used?\n",
    "* How is it implemented?\n",
    "* What are the drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d104c6b",
   "metadata": {},
   "source": [
    "[Non-maximum suppression](https://paperswithcode.com/method/non-maximum-suppression) (NMS) [1] is a popular technique used in object detection pipelines for handling duplicate or redundant bounding box predictions. Since object detection algorithms tend to produce more than one candidate bounding boxes for a single object, NMS is used to preserve only the _best_ bounding box per object using a overlap score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1023c79",
   "metadata": {},
   "source": [
    "#### History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79c1a9",
   "metadata": {},
   "source": [
    "Traditional object detection algorithms used an _exhaustive search_ method; by iteratively spanning the entire image space, any and all objects could in theory be precisely located. While this sliding window approach was successful in detecting objects at all sorts of unpredictable locations, exhaustive search was an extremely expensive algorithm to run — exploring and eliminating tens of thousands of candidate regions per image was far from efficient. Exhaustive search and other sliding window-based algorithms quickly became a relic and were replaced with more intuitive algorithms for object detection.\n",
    "\n",
    "Modern deep learning detection algorithms use a _region-based_ \"approximation\" algorithm to obtain object locations. Rather than scanning the entire image space iteratively, the image is split into sub-regions and analysed more efficiently using convolutional feature maps to determine whether or not an object is present. The [region-based convolutional neural networks](https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks) family, known as R-CNNs, revolutionised object detection and helped speed up efficiency to allow for real-time detection needed in today's applications of self-driving car technology. \n",
    "\n",
    "With these new methods brought a new set of challenges; R-CNN architectures often produced many candidate bounding boxes for each object detected. In order to eliminate the redundant boxes and preserve only one candidate bounding box per object, a new \"post-processing\" step needed to be defined. In 2009, Felzenszwalb et al., created _non-maxima suppression_ (NMS), a scoring metric that combined the [Intersection over Union](https://en.wikipedia.org/wiki/Jaccard_index) (IoU) and the predicted _confidence score_ into a single metric. NMS effectively discarded all but one bounding box for each object, eliminating the redundant bounding box problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d449f2b",
   "metadata": {},
   "source": [
    "### 1.2. Soft-NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4ee5e",
   "metadata": {},
   "source": [
    "Researchers began to notice that non-maximum suppression wasn't perfect; NMS tended to disregard otherwise valid objects of interest. These objects in particular were _occluded_ (obstructed or \"blocked\" by another object). Such objects have bound to have confidence scores that are lower than the average, thus a fixed threshold wouldn't cut it.\n",
    "\n",
    "Simply decreasing the confidence threshold to account for this could lead to a drop in average precision, increasing the number of _false positives_ (\"other\" bounding boxes belonging to the same object). Consequently, when the overlap threshold is increased, valid bounding boxes can be unintentionally suppressed (discarded), leaving some objects without a corresponding bounding box. This is especially relevant in the driving environment. As in high-density traffic conditions; two separate cars might share a high-degree of overlap. As a result, only one of the two overlapping bounding boxes would be preserved (the one with the greatest confidence threshold). In other words, both cars would be incorrectly assigned to the same bounding box.\n",
    "\n",
    "Soft-NMS by Bodla et al., seeks to address this problem in their cleverly-titled paper \"Improving Object Detection With One Line of Code\" [2]. Rather than suppressing boxes with a high-degree of overlap, we can instead decrease (\"decay\") their classification score. Re-running those candidates through a thresholding function could have one of two effects: those candidates no longer meet the threshold requirement and are therefore dropped, or, they are above the threshold and can therefore be kept. While the separate cars scenario might benefit from this simple case, as both boxes would be preserved, the more trivial case might be that the neighbouring redundant bounding box candidates would be incorrectly kept as well. Because of this likely scenario, Bodla et al. proposed a decay function that decreased the confidence scores linearly proportional to overlap amount. Therefore, candidate boxes that had a very high degree of overlap would experience a larger decay (greater penalty) than neighbouring bounding boxes without as much overlap. To better account for false positives, the Gaussian penalty function was introduced. By applying the update rule in each iteration, boxes with the highest degree of overlap are pruned and the number of false positives reduced at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f18eb",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c5907",
   "metadata": {},
   "source": [
    "### 2.1. Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7cf41",
   "metadata": {},
   "source": [
    "You are given a json file containing a list of predictions, containing `boxes` and `scores`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c74ef",
   "metadata": {},
   "source": [
    "You will leverage the `calculate_iou` function to calculate the Intersection Over Union (IoU) of these different predictions and implement the NMS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6ed45",
   "metadata": {},
   "source": [
    "#### IoU algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc17238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187ffa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(\n",
    "        bboxes1: tf.Tensor, \n",
    "        bboxes2: tf.Tensor\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Calculates the IoU score between a batch of bounding boxes.\n",
    "    \n",
    "    It is assumed that the number of bounding boxes N and M in each batch\n",
    "    are equal. It is also assumed that the coordinates are ordered as follows:\n",
    "        bbox := [y1, x1, y2, x2].\n",
    "    \n",
    "    :param bboxes1: the Nx4 bounding box coordinates.\n",
    "    :param bboxes2: the Mx4 bounding box coordinates.\n",
    "    :returns: iou, the intersection over union (IoU) scores between\n",
    "        the two batches of bounding boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### 1. Get the upper and lower (y, x) coordinate pairs\n",
    "    ymin = tf.math.reduce_max([bboxes1[..., 0], bboxes2[..., 0]])\n",
    "    xmin = tf.math.reduce_max([bboxes1[..., 1], bboxes2[..., 1]])\n",
    "    ymax = tf.math.reduce_min([bboxes1[..., 2], bboxes2[..., 2]])\n",
    "    xmax = tf.math.reduce_min([bboxes1[..., 3], bboxes2[..., 3]])\n",
    "    ### 2. Compute the following:\n",
    "    # Area of the intersection\n",
    "    intersection = tf.math.maximum(0, ymax - ymin) * tf.math.maximum(0, xmax - xmin)\n",
    "    # Area of the first batch\n",
    "    height = bboxes1[..., 2] - bboxes1[..., 0]\n",
    "    width = bboxes1[..., 3] - bboxes1[..., 1]\n",
    "    bboxes1_area = height * width\n",
    "    # Area of the second batch\n",
    "    height = bboxes2[..., 2] - bboxes2[..., 0]\n",
    "    width = bboxes2[..., 3] - bboxes2[..., 1]\n",
    "    bboxes2_area = height * width\n",
    "    # Area of the union\n",
    "    union = tf.cast(bboxes1_area + bboxes2_area - intersection, tf.dtypes.float32)\n",
    "    ### 3. Return the IoU scores\n",
    "    return tf.cast(intersection, tf.dtypes.float32) / (union + 1e-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278277f",
   "metadata": {},
   "source": [
    "To do so, you will need to:\n",
    "* compare each bounding box with all the other bounding boxes in the set\n",
    "* for each pair of bounding boxes, calculate the IoU and compare the scores\n",
    "* if the IoU is above the threshold, keep the box with the highest score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c857ca",
   "metadata": {},
   "source": [
    "#### NMS algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5379e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `nms.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa5cf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(\n",
    "        predictions: dict, \n",
    "        confidence_threshold: float=0.7, \n",
    "        iou_threshold: float=0.5\n",
    ") -> List[Union[List[int], float]]:\n",
    "    \"\"\"Performs non-maximum suppression (Felzenszwalb et al., 2008).\n",
    "    \n",
    "    Credit: https://bit.ly/3xQeFom\n",
    "    \n",
    "    :param predictions: the dict instance containing the ground truth\n",
    "        and predicted bounding box coordinates.\n",
    "    :param confidence_threshold: float, the confidence threshold, any\n",
    "        bounding box with score above this threshold is preserved.\n",
    "    :param iou_threshold: float, intersection over union threshold, any\n",
    "        bounding box with IoU score above this threshold is suppressed.\n",
    "    :returns filtered: the nested list of thresholded bounding box coordinates\n",
    "        and their corresponding confidence scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered = []\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    \n",
    "    ### 1. Discard all predictions below confidence threshold\n",
    "    # Getting indices of bounding boxes w.r.t. confidence in descending order\n",
    "    idxs = tf.argsort(predictions['scores'], direction='DESCENDING')\n",
    "    # Sorting the bounding boxes and their scores\n",
    "    scores = tf.gather(tf.cast(predictions['scores'], tf.dtypes.float32), idxs)\n",
    "    bboxes = tf.gather(predictions['boxes'], idxs)\n",
    "    # Keeping only the bounding boxes above the threshold\n",
    "    bboxes = bboxes[scores > confidence_threshold]\n",
    "    ### 2. For any remaining boxes\n",
    "    while len(bboxes) >= 2:\n",
    "        # Keep the bounding box with highest confidence score\n",
    "        filtered.append([\n",
    "            tf.squeeze(bboxes[0, :]).numpy().tolist(), tf.squeeze(scores[0]).numpy()])\n",
    "        # Update the remaining bounding boxes and their scores\n",
    "        other = bboxes[1:, :]\n",
    "        scores = scores[1:]\n",
    "        ### 3. Compute IoU scores between the bbox and all `m` other bboxes\n",
    "        # `broadcast_to` repeats the bbox coordinates `m` times\n",
    "        iou_scores = calculate_iou(\n",
    "                        bboxes1=tf.broadcast_to(bboxes[0, :], other.shape),\n",
    "                        bboxes2=other\n",
    "        )\n",
    "        # Keep bboxes whose IoU score is less than threshold\n",
    "        idxs_keep = tf.where(iou_scores <= iou_threshold)\n",
    "        bboxes = tf.gather(other, idxs_keep)\n",
    "        scores = tf.gather(scores, idxs_keep)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455225fa",
   "metadata": {},
   "source": [
    "You can run `python nms.py` to check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8917d70",
   "metadata": {},
   "source": [
    "#### Testing the NMS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d25933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetching the test image filename\n",
    "test_image = 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "166fcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `nms.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19143342",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetching the predicted data\n",
    "with open('data/predictions_nms.json', 'r') as f:\n",
    "    predictions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97c32260",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.data.Dataset.from_tensors(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8079970",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtaining the 'picked' bounding boxes after performing NMS\n",
    "output = nms(iter(predictions).get_next(), confidence_threshold=0.0, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857307d4",
   "metadata": {},
   "source": [
    "#### Evaluating our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0117f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8350a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(output):\n",
    "    truth = np.load(os.path.join(DIR_SRC, 'nms.npy'), allow_pickle=True)\n",
    "    assert np.array_equal(truth, np.array(output, dtype=\"object\")), 'The NMS implementation is wrong'\n",
    "    print('The NMS implementation is correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "516e7d91",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The NMS implementation is wrong",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m preds_nms \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m preds_nms \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(preds_nms)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcheck_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreds_nms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mcheck_results\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_results\u001b[39m(output):\n\u001b[1;32m      2\u001b[0m     truth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DIR_SRC, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnms.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(truth, np\u001b[38;5;241m.\u001b[39marray(output, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe NMS implementation is wrong\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe NMS implementation is correct!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The NMS implementation is wrong"
     ]
    }
   ],
   "source": [
    "### Checking our results against Udacity's\n",
    "with open(os.path.join(DIR_SRC, 'predictions_nms.json')) as f:\n",
    "    preds_nms = json.load(f)\n",
    "    preds_nms = tf.data.Dataset.from_tensors(preds_nms)\n",
    "    check_results(nms(iter(preds_nms).get_next(), confidence_threshold=0.0, iou_threshold=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382cf557",
   "metadata": {},
   "source": [
    "**UPDATE 2022-09-24 14:35 PT**: Looks like our updated `nms` function results aren't matching Udacity's. Let's investigate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86164215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[107, 107, 192, 148], 0.8147568], [[101, 90, 206, 141], 0.7962376]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test = nms(iter(preds_nms).get_next(), confidence_threshold=0.0, iou_threshold=0.5)\n",
    "output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22568d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([107, 107, 192, 148]), 0.8147568],\n",
       "       [list([101, 90, 206, 141]), 0.7962376]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Previewing the output results\n",
    "np.array(output_test, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b82e59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([107, 107, 192, 148]), 0.814756824762981],\n",
       "       [list([101, 90, 206, 141]), 0.7962375654802895]], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Previewing the results from Udacity\n",
    "np.load(os.path.join(DIR_SRC, 'nms.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38d4b9",
   "metadata": {},
   "source": [
    "So it seems as if this is a just floating-point precision error. Both the actual and expected results match in terms of bounding box coordinates and the confidence scores. However, the results from our `nms` function indicate that the confidence scores have been truncated (likely due to the `tf.dtypes.float32` format).\n",
    "\n",
    "We can safely conclude that our `nms` method functions as intended.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f7d7b",
   "metadata": {},
   "source": [
    "**UPDATE 2022-09-24 14:48 PT**: Something I noticed while refactoring `nms`... previously in the `calculate_iou` function I implemented in Numpy (see commit #[34a296d](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/34a296d7ae16f1f2e97089137c000916e3482b06/1-Object-Detection-in-Urban-Environments/Exercises/1-5-1-Non-Maximum-Suppression/2022-09-21-Non-Maximum-Suppression.ipynb)), I was unintentionally calculating the area of intersection of _one_ and not _all_ boxes. In otherwords, I was only performing a `reduce_min` and `reduce_max` over the batch of bounding boxes (returning only one box's coordinates to compute the area of). \n",
    "\n",
    "I believe this is not the correct approach and that I should be instead computing the element-wise `min` and `max` for each coordinate. Therefore, the area of intersection between each bounding box can be calculated as intended. To fix this, I will need to change the `reduce_max`/`reduce_min` operations to their element-wise equivalents (`maximum`/`minimum`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebacd77",
   "metadata": {},
   "source": [
    "#### Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dfa5c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': [[793, 1134, 1001, 1718],\n",
       "   [737, 0, 898, 260],\n",
       "   [763, 484, 878, 619],\n",
       "   [734, 0, 1114, 277],\n",
       "   [820, 1566, 974, 1914],\n",
       "   [762, 951, 844, 1175],\n",
       "   [748, 197, 803, 363]],\n",
       "  'classes': [1, 1, 1, 1, 1, 1, 1],\n",
       "  'filename': 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fetching the ground-truth bounding boxes\n",
    "with open(os.path.join(DIR_SRC, 'ground_truths.json'), 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d42edff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From J. Moran's `2022-08-01-Data-Acquisition-Visualisation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2c21fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_to_rectangle(bbox: List[int], **params) -> mpl.patches.Rectangle:\n",
    "    \"\"\"Renders bounding boxes as Matplotlib patches.\n",
    "    \n",
    "    Bounding box coordinate pairs form a single \n",
    "    orthogonal rectangle about the x-y plane.\n",
    "    The upper-left coordinates are given by (y1, x1)\n",
    "    and the lower-left coordinates by (y2, x2).\n",
    "    \n",
    "    Note that this assumes a bounding box coordinate ordering of:\n",
    "        bbox := [y1, x1, y2, x2].\n",
    "    \n",
    "    :param bbox: list of bounding box coordinates\n",
    "    :param **params: dict-like object of `matplotlib.patches.Rectangle`\n",
    "        parameters, e.g., `linewidth`, `color`, and `linestyle`.\n",
    "    :returns: matplotlib Rectangle patch instance\n",
    "    \"\"\"\n",
    "    \n",
    "    return patches.Rectangle(xy=(bbox[1], bbox[0]), \n",
    "                             width=(bbox[3]-bbox[1]), height=(bbox[2]-bbox[0]), \n",
    "                             **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a63f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box colours\n",
    "class_colourmap = {1:'cyan',        # Ground-truth \n",
    "                   2:'magenta',     # Predicted\n",
    "                   3:'yellow'}      # NMS picked\n",
    "default = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab9650b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectangle patch parameters\n",
    "rect_params = {'fill': False,\n",
    "               'edgecolor': default,\n",
    "               'linewidth': 2,\n",
    "               'linestyle': '--'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29172a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,24))\n",
    "image = Image.open(os.path.join(DIR_SRC, test_image))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(image)\n",
    "\n",
    "### Plot the ground-truth bounding boxes\n",
    "for j, bbox in enumerate(ground_truth[0]['boxes']):\n",
    "    # Get bounding box class\n",
    "    cls_bbox = ground_truth[0]['classes'][j]\n",
    "    # Update bbox color\n",
    "    rect_params['edgecolor'] = class_colourmap.get(cls_bbox, 'red')\n",
    "    ax.add_patch(bbox_to_rectangle(bbox, **rect_params))\n",
    "### Plot the predicted bounding boxes\n",
    "for j, bbox in enumerate(iter(predictions).get_next()['boxes'].numpy()):\n",
    "    # Here we assume all bboxes are of same class;\n",
    "    # instead use colour to indicate ground truth/predicted/picked\n",
    "    cls_bbox = 2\n",
    "    # Update bbox color\n",
    "    rect_params['edgecolor'] = class_colourmap.get(cls_bbox, 'red')\n",
    "    ax.add_patch(bbox_to_rectangle(bbox, **rect_params))\n",
    "### Plot the picked bounding boxes preserved with NMS\n",
    "for j, bbox in enumerate(output):\n",
    "    # Here we assume all bboxes are of same class;\n",
    "    # instead use colour to indicate ground truth/predicted/picked\n",
    "    cls_bbox = 3\n",
    "    # Update bbox color\n",
    "    rect_params['edgecolor'] = class_colourmap.get(cls_bbox, 'red')\n",
    "    ax.add_patch(bbox_to_rectangle(bbox[0], **rect_params))\n",
    "ax.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813f046",
   "metadata": {},
   "source": [
    "Here we see that NMS didn't do such a good job of picking one bounding box per object ('picked' boxes shown in yellow). We'll have to explore this in more depth..\n",
    "\n",
    "Wondering if it is possible that only the bounding boxes belonging to a single object should be passed into NMS to be thresholded..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45716d",
   "metadata": {},
   "source": [
    "### 2.2. Soft-NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67af7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_nms(predictions: dict):\n",
    "    \"\"\"Soft-NMS algorithm as in Bodla et al., 2017.\n",
    "    \n",
    "    :param predictions: the dict instance containing the ground truth\n",
    "        and predicted bounding box coordinates.\n",
    "    :returns filtered: the list of thresholded bounding boxes and their\n",
    "        computed Soft-NMS scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a7cf19",
   "metadata": {},
   "source": [
    "## 3. Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd601efd",
   "metadata": {},
   "source": [
    "##### Alternatives\n",
    "* Use the [Soft-NMS](https://arxiv.org/abs/1704.04503) algorithm to handle occluded objects\n",
    "\n",
    "##### Extensions of task\n",
    "* Apply NMS to a object detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03046d3c",
   "metadata": {},
   "source": [
    "## 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e255e",
   "metadata": {},
   "source": [
    "- [ ] Compare NMS and Soft-NMS on images with occluded objects;\n",
    "- [ ] Add NMS/Soft-NMS to an object detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651ef91",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eaaac",
   "metadata": {},
   "source": [
    "This assignment was prepared by Thomas Hossler et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "[1] Felzenszwalb, P. F., et al. Object Detection with Discriminatively Trained Part-Based Models. IEEE Transactions on Pattern Analysis and Machine Intelligence. 32(9):1627-1645. 2010. [doi:10.1109/TPAMI.2009.167](https://ieeexplore.ieee.org/document/5255236).\n",
    "\n",
    "[2] Bodla, N. et al. Soft-NMS — Improving Object Detection With One Line of Code. arXiv. 2017. [doi:10.48550/ARXIV.1704.04503](https://arxiv.org/abs/1704.04503).\n",
    "\n",
    "\n",
    "\n",
    "Further reading:\n",
    "* Uijlings, J.R.R., et al. Selective Search for Object Recognition. International Journal of Computer Vision, 104:154–171. 2013. [doi:10.1007/s11263-013-0620-5](https://doi.org/10.1007/s11263-013-0620-5).\n",
    "\n",
    "* Ren, S., et al., Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems, vol. 28. 2015. [doi:10.48550/ARXIV.1506.01497](https://arxiv.org/abs/1506.01497).\n",
    "\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [Selective Search for Object Recognition by S. Smith | CS231B at Stanford University](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/ssearch_schuyler.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

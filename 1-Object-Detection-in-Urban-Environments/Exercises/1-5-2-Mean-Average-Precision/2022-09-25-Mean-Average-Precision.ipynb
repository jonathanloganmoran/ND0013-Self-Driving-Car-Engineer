{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea18cb74",
   "metadata": {},
   "source": [
    "# Exercise 1.5.2 - Mean Average Precision (mAP)\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa47713",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8d536",
   "metadata": {},
   "source": [
    "* Implement the Mean Average Precision (mAP) metric;\n",
    "* To do so, create a function to calculate the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) scores;\n",
    "* Visualise the Precision-Recall (PR) curve and smoothed PR curve.\n",
    "* Compute the mAP over the provided test data (a frame from the [Waymo Open Dataset](https://waymo.com/open))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee8638",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38526881",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74593795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5573eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce528c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ee43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fef4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48a3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce635c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9911f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79be3d",
   "metadata": {},
   "source": [
    "### 1.1. Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc19c3",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d16414",
   "metadata": {},
   "source": [
    "[Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\)#Mean_average_precision) (mAP) is a widely-used accuracy metric for object detection models. As the name entails, mAP is simply the average of the [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\)#Average_precision) (AP) computed with respect to all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108948c",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbb8ae",
   "metadata": {},
   "source": [
    "[Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) and [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) (sometimes called [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)) are two single-valued metrics often computed over the entire document set (in our case, the set of all predictions made). We can, however, also compute precision and recall at every step along that interval. \n",
    "\n",
    "In other words, we can compute these two metrics cumulatively, prediction-by-prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa2550",
   "metadata": {},
   "source": [
    "##### Precision-Recall (PR) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c004d",
   "metadata": {},
   "source": [
    "We can use the cumulative precision and recall calculations to plot a Precision-Recall curve. In order to generate the points along this curve, we have to calculate the _true positive_, _false positive_, _true negative_ and _false negative_ rate over a set of of predictions and their ground truth labels:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathrm{Precision} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} = \\frac{\\mathrm{TP}}{\\mathrm{all \\ detections}}, &\n",
    "    \\mathrm{Recall} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} = \\frac{\\mathrm{TP}}{\\mathrm{all \\ ground \\ truths}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Often times the resulting PR curve will have extremes (jagged peaks) and result in a less-accurate area under the curve (AUC) calculation. To fix this, two common [interpolation](https://en.wikipedia.org/wiki/Interpolation) techniques are used to smooth out the PR curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affca413",
   "metadata": {},
   "source": [
    "###### Interpolation\n",
    "\n",
    "The [11-Point Interpolation](https://github.com/rafaelpadilla/Object-Detection-Metrics#11-point-interpolation) technique pioneered by Everingham et al., 2010 [1] attempts to summarise the shape of the Precision $x$ Recall curve by averaging the precision values at a set of eleven equally-spaced recall levels in the range $[0, 1]$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{AP} &= \\frac{1}{11} \\sum_{r \\ \\in \\ \\{0, 0.1, .., 1\\}} \\rho_{\\mathrm{interp}}(r), \\quad\n",
    "& \\rho_{\\mathrm{interp}}(r) &= \\max_{\\bar{r}:\\bar{r} \\geq r}(\\tilde{r}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\rho(\\tilde{r})$ is the measured precision at recall $\\tilde{r}$. The precision value at each observed point is not used to compute the PR curve, instead the $AP$ value is obtained by interpolation the precision only at these $r=11$ levels. The precision is determined to be the maximum precision value whose recall is greater than $r$.\n",
    "\n",
    "The [All-Points Interpolation](https://github.com/rafaelpadilla/Object-Detection-Metrics#interpolating-all-points) [2] method extends the 11-Points method to a set of recall levels $n$ equal to the number of observations along the Precision $x$ Recall curve, such that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{AP} &= \\frac{1}{n} \\sum_{n=0} \\left(r_{n+1} - r_{n}\\right) \\rho_{\\mathrm{interp}}(r_{n+1}), \\quad\n",
    "& \\rho_{\\mathrm{interp}}(r_{n+1}) &= \\max_{\\bar{r}:\\bar{r} \\geq r_{n+1}}(\\tilde{r}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $p(\\tilde{r})$ is the measured precision at recall $\\tilde{r}$. Instead of using precision observed at only $r = 11$ points, we can now obtain the average precision (AP) by interpolating at each level $r$ and taking the maximum precision value whose recall is greater than $r + 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab2bf8",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b6550",
   "metadata": {},
   "source": [
    "### 2.1. Precision-Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee9966",
   "metadata": {},
   "source": [
    "#### IoU algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ce83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From J. Moran's `2022-07-25-Choosing-Metrics-IoU.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5b7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoU:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _overlapping_rectangles(bbox1: List[int], bbox2: List[int]) -> bool:\n",
    "        \"\"\"Returns True if the bounding boxes overlap.\n",
    "        \n",
    "        Two bounding boxes overlap if their area is positive and non-zero.\n",
    "        \n",
    "        :param bbox1: 1x4 list of x-y coordinates forming a rectangle.\n",
    "        :param bbox2: 1x4 list of x-y coordinates forming a rectangle.\n",
    "        :returns: bool, whether or not the two rectangles overlap.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Check if bounding boxes do not overlap\n",
    "        if (\n",
    "            # (a) First bbox lower edge is GEQ second bbox upper edge\n",
    "            (min(bbox1[1], bbox2[1]) >= max(bbox1[3], bbox2[3])) or\n",
    "            # (b) First bbox right edge is LEQ second bbox left edge\n",
    "            (min(bbox1[2], bbox2[2]) <= max(bbox1[0], bbox2[0])) or\n",
    "            # (c) First bbox left edge is GEQ second bbox right edge\n",
    "            (min(bbox1[0], bbox2[0]) >= max(bbox1[2], bbox2[2])) or\n",
    "            # (d) First bbox upper edge is LEQ second bbox lower edge\n",
    "            (min(bbox1[3], bbox2[3]) <= max(bbox1[1], bbox2[1]))\n",
    "        ):\n",
    "            return False\n",
    "        # 2. Check if intersection area is larger than 0\n",
    "        else:\n",
    "            x_inter1 = max(bbox1[0], bbox2[0])\n",
    "            y_inter1 = max(bbox1[1], bbox2[1])\n",
    "            x_inter2 = min(bbox1[2], bbox2[2])\n",
    "            y_inter2 = min(bbox1[3], bbox2[3])\n",
    "            # Overlapping region must have positive area\n",
    "            w_inter = max(0, (x_inter2 - x_inter1))\n",
    "            h_inter = max(0, (y_inter2 - y_inter1))\n",
    "            if (w_inter * h_inter) > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _calculate_iou(self, gt_bbox: List[int], pred_bbox: List[int]) -> float:\n",
    "        \"\"\"Calculates the IoU score for a single pair of bounding boxes.\n",
    "\n",
    "        :param gt_bbox: 1x4 list of ground-truth coordinates,\n",
    "        :param pred_bbox: 1x4 list of predicted coordinates,\n",
    "        returns: iou, the pairwise IoU score between the two bounding boxes.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Check if bounding boxes overlap\n",
    "        if not self._overlapping_rectangles(gt_bbox, pred_bbox):\n",
    "            return 0.0\n",
    "        else:\n",
    "            # 2. Find the coordinates of the area of intersection\n",
    "            #    2a. The upper-left coordinate\n",
    "            x_inter1 = max(gt_bbox[0], pred_bbox[0])\n",
    "            y_inter1 = max(gt_bbox[1], pred_bbox[1])\n",
    "            #    2b. The lower-right coordinate\n",
    "            x_inter2 = min(gt_bbox[2], pred_bbox[2])\n",
    "            y_inter2 = min(gt_bbox[3], pred_bbox[3])\n",
    "            # 3. Calculate the area of intersection\n",
    "            w_inter = abs(x_inter2 - x_inter1)\n",
    "            h_inter = abs(y_inter2 - y_inter1)\n",
    "            area_inter = w_inter * h_inter\n",
    "            # 4. Find the coordinates of the area of union\n",
    "            #    4a. The height and width of the first box \n",
    "            w_union1 = abs(gt_bbox[2] - gt_bbox[0])\n",
    "            h_union1 = abs(gt_bbox[3] - gt_bbox[1])\n",
    "            #    4b. The height and width of the second box\n",
    "            w_union2 = abs(pred_bbox[2] - pred_bbox[0])\n",
    "            h_union2 = abs(pred_bbox[3] - pred_bbox[1])\n",
    "            # 5. Calculate the area of union\n",
    "            area_union = (w_union1 * h_union1) + (w_union2 * h_union2)\n",
    "            area_union -= area_inter\n",
    "            # 6. Calculate the resulting IoU score\n",
    "            iou = float(area_inter) / float(area_union)\n",
    "        return iou\n",
    "\n",
    "    def calculate_ious(self, gt_bboxes: List[List[int]], \n",
    "                       pred_bboxes: List[List[int]]) -> List[float]:\n",
    "        \"\"\"Calculates the IoU scores for all bounding box pairs.\n",
    "\n",
    "        :param gt_bboxes: Nx4 list of ground-truth coordinates,\n",
    "        :param pred_bboxes: Mx4 list of predicted coordinates,\n",
    "        returns: iou, NxM list of pairwise IoU scores.\n",
    "        \"\"\"\n",
    "\n",
    "        # Allocate a NumPy array of zeros to store the IoU scores\n",
    "        ious = np.zeros((gt_bboxes.shape[0], pred_bboxes.shape[0]))\n",
    "        # For each ground-truth bounding box\n",
    "        for i, gt_bbox in enumerate(gt_bboxes):\n",
    "            # Calculate the IoU score w.r.t the entire inference set\n",
    "            for j, pred_bbox in enumerate(pred_bboxes):\n",
    "                ious[i,j] = self._calculate_iou(gt_bbox, pred_bbox)\n",
    "        return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2797352",
   "metadata": {},
   "source": [
    "To do so, you will first have to create the Precision-Recall (PR) curve. Once this curve is created, you need to create the smoothed version as discussed in the lesson. Finally you can use this smoothed version to calculate the mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471630a1",
   "metadata": {},
   "source": [
    "#### Precision and Recall algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bba5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From J. Moran's `2022-07-25-Choosing-Metrics-IoU.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9289cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionRecall:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def _compute_class_metrics(self, ious: List[float], gt_classes: List[int],\n",
    "                               pred_classes: List[int], iou_threshold: float) -> List[dict]:\n",
    "        \"\"\"Computes the classification metrics for a multi-class dataset.\n",
    "        \n",
    "        Dataset contains pairwse IoU scores for the bounding box prediction problem.\n",
    "        \n",
    "        :param ious: NxM list of pairwise IoU scores\n",
    "        :param gt_classes: 1xN list of ground truth class labels\n",
    "        :param pred_classes: 1xM list of predicted class labels\n",
    "        :param iou_threshold: float threshold, predictions 'valid' above this value.\n",
    "        :returns: list of dict objects containing the per-class metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store per-class metrics\n",
    "        cls_metrics = []\n",
    "        # Convert matrix into pandas DataFrame for easier slicing/indexing\n",
    "        df = pd.DataFrame(data=ious, index=gt_classes, columns=pred_classes)\n",
    "        # Compute per-class metrics\n",
    "        for cls in np.unique(gt_classes):\n",
    "            # Get all preds for gt cls label\n",
    "            cls_df = df.loc[[cls],:]    # Preserves DataFrame structure\n",
    "            try:\n",
    "                # Get all gt-pred pairs with matching class labels\n",
    "                cls_data = cls_df[cls]\n",
    "                # Count number of TP (correct class, IoU > 0.5) predictions\n",
    "                true_positives = np.count_nonzero(cls_data.where(cls_data > 0.5).fillna(0))\n",
    "            except KeyError as k:\n",
    "                print(f\"No predicted bounding boxes for class label '{k}'\")\n",
    "            # Count number of TN (i.e., no bounding box to predict, N/A for our data)\n",
    "            true_negatives = 0\n",
    "            # Count number of FN (incorrect class, IoU < 0.5) predictions\n",
    "            false_negatives = np.count_nonzero(cls_df.where(\n",
    "                                cls_df < 0.5).drop(columns=cls, errors='ignore').fillna(0))\n",
    "            # Count number of FP (incorrect class, IoU > 0.5) predictions\n",
    "            false_positives = np.count_nonzero(cls_df.where(\n",
    "                                cls_df > 0.5).drop(columns=cls, errors='ignore').fillna(0))\n",
    "            # Calculate precsion/recall for each class and store in dict\n",
    "            cls_dict = {'TN': true_negatives,\n",
    "                        'TP': true_positives,\n",
    "                        'FN': false_negatives,\n",
    "                        'FP': false_positives\n",
    "                        }\n",
    "            cls_metrics.append(cls_dict)\n",
    "        return cls_metrics\n",
    "\n",
    "    def precision_recall(self, ious: List[float], gt_classes: List[int], \n",
    "                         pred_classes: List[int], iou_threshold:float=0.5) -> (float, float):\n",
    "        \"\"\"Calculates the precision and recall metrics.\n",
    "        \n",
    "        Dataset contains pairwse IoU scores for the bounding box prediction problem.\n",
    "        Columns are the predicted class labels, rows are the ground truth class labels.\n",
    "        \n",
    "        :param ious: NxM list of pairwise IoU scores\n",
    "        :param gt_classes: 1xN list of ground truth class labels\n",
    "        :param pred_classes: 1xM list of predicted class labels\n",
    "        :param iou_threshold: float threshold, predictions 'valid' above this value.\n",
    "        :returns: (precision, recall), the two classification metric values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute per-class metrics\n",
    "        cls_metrics = self._compute_class_metrics(ious, gt_classes, pred_classes, iou_threshold)\n",
    "        # Store total metrics\n",
    "        TN_total, TP_total = 0, 0\n",
    "        FN_total, FP_total = 0, 0\n",
    "        for c in cls_metrics:\n",
    "            TN_total += c['TN']\n",
    "            TP_total += c['TP']\n",
    "            FN_total += c['FN']\n",
    "            FP_total += c['FP']\n",
    "        # Print total metrics\n",
    "        print(\"TP:\", TP_total, \"TN:\", TN_total, \"\\nFP:\", FP_total, \"FN:\", FN_total)\n",
    "        # Compute combined metrics\n",
    "        precision = TP_total / float(TP_total + FP_total)\n",
    "        recall = TP_total / float(TP_total + FN_total)\n",
    "        return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7615c72",
   "metadata": {},
   "source": [
    "#### Collecting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea93ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc187d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(nms: bool=False):\n",
    "    \"\"\"Simple wrapper function to get data.\n",
    "    \n",
    "    :param nms: boolean, returns data for NMS testing if True.\n",
    "    \"\"\"\n",
    "    \n",
    "    ext = ''\n",
    "    if nms:\n",
    "        ext = '_nms'\n",
    "    with open(os.path.join(DIR_SRC, 'ground_truths.json')) as f:\n",
    "        ground_truth = json.load(f)\n",
    "    with open(os.path.join(DIR_SRC, f'predictions{ext}.json')) as f:\n",
    "        predictions = json.load(f)\n",
    "        \n",
    "    return ground_truth, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e31e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IMPLEMENT THIS SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6ff6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth, predictions = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "210bb483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': [[793, 1134, 1001, 1718],\n",
       "   [737, 0, 898, 260],\n",
       "   [763, 484, 878, 619],\n",
       "   [734, 0, 1114, 277],\n",
       "   [820, 1566, 974, 1914],\n",
       "   [762, 951, 844, 1175],\n",
       "   [748, 197, 803, 363]],\n",
       "  'classes': [1, 1, 1, 1, 1, 1, 1],\n",
       "  'filename': 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a3b09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': [[783, 1104, 1011, 1700],\n",
       "   [717, 0, 908, 280],\n",
       "   [734, 0, 1100, 240],\n",
       "   [744, 0, 1080, 258],\n",
       "   [753, 474, 868, 609],\n",
       "   [830, 1500, 1004, 1914],\n",
       "   [758, 192, 811, 364],\n",
       "   [810, 1576, 973, 1909]],\n",
       "  'classes': [1, 1, 1, 2, 1, 1, 1, 3],\n",
       "  'scores': [0.32, 0.97, 0.51, 0.44, 0.12, 0.99, 0.73, 0.95],\n",
       "  'filename': 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09b480cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetching bounding box data from parsed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1316d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'\n",
    "gt_bboxes = [g['boxes'] for g in ground_truth if g['filename'] == filename][0]\n",
    "gt_bboxes = np.array(gt_bboxes)\n",
    "gt_classes = [g['classes'] for g in ground_truth if g['filename'] == filename][0]\n",
    "\n",
    "\n",
    "pred_bboxes = [p['boxes'] for p in predictions if p['filename'] == filename][0]\n",
    "pred_bboxes = np.array(pred_bboxes) # pred_boxes -> pred_bboxes\n",
    "pred_classes = [p['classes'] for p in predictions if p['filename'] == filename][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92418d4",
   "metadata": {},
   "source": [
    "#### Obtaining IoU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b5ec581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84313051, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.23860974, 0.        , 0.15167262],\n",
       "       [0.        , 0.78272251, 0.4243356 , 0.44735183, 0.        ,\n",
       "        0.        , 0.0760787 , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.73221757,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.43601527, 0.83450504, 0.82356071, 0.        ,\n",
       "        0.        , 0.04100263, 0.        ],\n",
       "       [0.12221933, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.66359447, 0.        , 0.89506693],\n",
       "       [0.02888778, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.07864588, 0.02499868, 0.03628478, 0.        ,\n",
       "        0.        , 0.69320713, 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the pair-wise IoU scores\n",
    "ious = IoU().calculate_ious(gt_bboxes, pred_bboxes)\n",
    "ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11a38e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualising our pairwise IoU scores in a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8142f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5766b2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.843131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782723</td>\n",
       "      <td>0.424336</td>\n",
       "      <td>0.447352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076079</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436015</td>\n",
       "      <td>0.834505</td>\n",
       "      <td>0.823561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078646</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.036285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693207</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         1         1         2         1         1         1  \\\n",
       "1  0.843131  0.000000  0.000000  0.000000  0.000000  0.238610  0.000000   \n",
       "1  0.000000  0.782723  0.424336  0.447352  0.000000  0.000000  0.076079   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.732218  0.000000  0.000000   \n",
       "1  0.000000  0.436015  0.834505  0.823561  0.000000  0.000000  0.041003   \n",
       "1  0.122219  0.000000  0.000000  0.000000  0.000000  0.663594  0.000000   \n",
       "1  0.028888  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.078646  0.024999  0.036285  0.000000  0.000000  0.693207   \n",
       "\n",
       "          3  \n",
       "1  0.151673  \n",
       "1  0.000000  \n",
       "1  0.000000  \n",
       "1  0.000000  \n",
       "1  0.895067  \n",
       "1  0.000000  \n",
       "1  0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=ious, index=gt_classes, columns=pred_classes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03bde1",
   "metadata": {},
   "source": [
    "#### Obtaining precision and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "538697e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 6 TN: 0 \n",
      "FP: 2 FN: 3\n"
     ]
    }
   ],
   "source": [
    "### Testing my Precision/Recall algorithm\n",
    "precision, recall = PrecisionRecall().precision_recall(ious, gt_classes, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetching NMS evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f969295",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth, predictions = get_data(nms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20835b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': [[793, 1134, 1001, 1718],\n",
       "   [737, 0, 898, 260],\n",
       "   [763, 484, 878, 619],\n",
       "   [734, 0, 1114, 277],\n",
       "   [820, 1566, 974, 1914],\n",
       "   [762, 951, 844, 1175],\n",
       "   [748, 197, 803, 363]],\n",
       "  'classes': [1, 1, 1, 1, 1, 1, 1],\n",
       "  'filename': 'segment-1231623110026745648_480_000_500_000_with_camera_labels_38.png'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "703c7dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'boxes': [[106, 108, 198, 149],\n",
       "  [106, 94, 195, 141],\n",
       "  [105, 104, 202, 153],\n",
       "  [107, 107, 192, 148],\n",
       "  [104, 102, 196, 155],\n",
       "  [102, 91, 198, 157],\n",
       "  [91, 94, 200, 143],\n",
       "  [101, 103, 192, 153],\n",
       "  [101, 90, 206, 141]],\n",
       " 'scores': [0.07409888441353274,\n",
       "  0.6034416172735038,\n",
       "  0.07242464990801578,\n",
       "  0.814756824762981,\n",
       "  0.12201349036245734,\n",
       "  0.47478430999733756,\n",
       "  0.06635549149848208,\n",
       "  0.6067337424650986,\n",
       "  0.7962375654802895]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2a330",
   "metadata": {},
   "source": [
    "You also have to create a visualization of the PR and smoothed PR curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ffe3d5",
   "metadata": {},
   "source": [
    "Make sure to check the `Desktop` to see your visualization when running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(output):\n",
    "    round_output = np.round(output * 1e4) / 1e4\n",
    "    assert round_output == 0.7286, 'Something is wrong with the mAP calculation'\n",
    "    print('mAP calculation is correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eafb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results(mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138ad53",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015d667",
   "metadata": {},
   "source": [
    "To create the PR curve, you need to sort the predictions based on their confidence score and calculate the precision and recall for each subset of the predictions, as explained in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17811f",
   "metadata": {},
   "source": [
    "To make your life easier, you can hard code the smoothed PR curve based on the PR curve, but \n",
    "you should think of a scripted version of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b641e",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a2cdd",
   "metadata": {},
   "source": [
    "This assignment was prepared by Thomas Hossler et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "References\n",
    "* [1] Everingham, M., et al., The PASCAL Visual Object Classes (VOC) Challenge. International Journal of Computer Vision. 88:303â€“338. 2010. [doi:10.1007/s11263-009-0275-4](https://doi.org/10.1007/s11263-009-0275-4).\n",
    "\n",
    "* [2] Padilla, R., et al., A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit. Electronics. 10(3):279. [doi:10.3390/electronics10030279](https://www.mdpi.com/2079-9292/10/3/279).\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [A Coder's Guide to IoU, Non-Max Suppression and Mean Average Precision by Vijayabhaskar J. | Medium](https://vijayabhaskar96.medium.com/practitioners-guide-to-iou-non-max-suppression-and-mean-average-precision-e09de73a2bd8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

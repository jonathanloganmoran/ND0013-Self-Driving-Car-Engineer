{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3aaad3",
   "metadata": {},
   "source": [
    "# Exercise 1.5.2 - Mean Average Precision (mAP)\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c94bbdf",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a592f",
   "metadata": {},
   "source": [
    "* Implement the Mean Average Precision (mAP) metric;\n",
    "* To do so, create a function to calculate the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) scores;\n",
    "* Visualise the Precision-Recall (PR) curve and smoothed PR curve.\n",
    "* Compute the mAP over the provided test data (a frame from the [Waymo Open Dataset](https://waymo.com/open))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e7aa4",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9f7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c478d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b26a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b691df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a326b6",
   "metadata": {},
   "source": [
    "### 1.1. Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcc9e3",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09d23e",
   "metadata": {},
   "source": [
    "[Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\)#Mean_average_precision) (mAP) is a widely-used accuracy metric for object detection models. As the name entails, mAP is simply the average of the [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\)#Average_precision) (AP) computed with respect to all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b6dea",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086d732",
   "metadata": {},
   "source": [
    "[Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) and [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) (sometimes called [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)) are two single-valued metrics often computed over the entire document set (in our case, the set of all predictions made). We can, however, also compute precision and recall at every step along that interval. \n",
    "\n",
    "In other words, we can compute these two metrics cumulatively, prediction-by-prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08797e",
   "metadata": {},
   "source": [
    "##### Precision-Recall (PR) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93da455",
   "metadata": {},
   "source": [
    "We can use the cumulative precision and recall calculations to plot a Precision-Recall curve. In order to generate the points along this curve, we have to calculate the _true positive_, _false positive_, _true negative_ and _false negative_ rate over a set of of predictions and their ground truth labels:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathrm{Precision} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} = \\frac{\\mathrm{TP}}{\\mathrm{all \\ detections}}, &\n",
    "    \\mathrm{Recall} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} = \\frac{\\mathrm{TP}}{\\mathrm{all \\ ground \\ truths}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Often times the resulting PR curve will have extremes (jagged peaks) and result in a less-accurate area under the curve (AUC) calculation. To fix this, two common [interpolation](https://en.wikipedia.org/wiki/Interpolation) techniques are used to smooth out the PR curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e66e4",
   "metadata": {},
   "source": [
    "###### Interpolation\n",
    "\n",
    "The [11-Point Interpolation](https://github.com/rafaelpadilla/Object-Detection-Metrics#11-point-interpolation) technique pioneered by Everingham et al., 2010 [1] attempts to summarise the shape of the Precision $x$ Recall curve by averaging the precision values at a set of eleven equally-spaced recall levels in the range $[0, 1]$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{AP} &= \\frac{1}{11} \\sum_{r \\ \\in \\ \\{0, 0.1, .., 1\\}} \\rho_{\\mathrm{interp}}(r), \\quad\n",
    "& \\rho_{\\mathrm{interp}}(r) &= \\max_{\\bar{r}:\\bar{r} \\geq r}(\\tilde{r}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\rho(\\tilde{r})$ is the measured precision at recall $\\tilde{r}$. The precision value at each observed point is not used to compute the PR curve, instead the $AP$ value is obtained by interpolation the precision only at these $r=11$ levels. The precision is determined to be the maximum precision value whose recall is greater than $r$.\n",
    "\n",
    "The [All-Points Interpolation](https://github.com/rafaelpadilla/Object-Detection-Metrics#interpolating-all-points) [2] method extends the 11-Points method to a set of recall levels $n$ equal to the number of observations along the Precision $x$ Recall curve, such that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{AP} &= \\frac{1}{n} \\sum_{n=0} \\left(r_{n+1} - r_{n}\\right) \\rho_{\\mathrm{interp}}(r_{n+1}), \\quad\n",
    "& \\rho_{\\mathrm{interp}}(r_{n+1}) &= \\max_{\\bar{r}:\\bar{r} \\geq r_{n+1}}(\\tilde{r}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $p(\\tilde{r})$ is the measured precision at recall $\\tilde{r}$. Instead of using precision observed at only $r = 11$ points, we can now obtain the average precision (AP) by interpolating at each level $r$ and taking the maximum precision value whose recall is greater than $r + 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca0baf",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8eecc9",
   "metadata": {},
   "source": [
    "### 2.1. Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ab943",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f49daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(gt_bbox, pred_bbox):\n",
    "    \"\"\"\n",
    "    calculate iou \n",
    "    args:\n",
    "    - gt_bbox [array]: 1x4 single gt bbox\n",
    "    - pred_bbox [array]: 1x4 single pred bbox\n",
    "    returns:\n",
    "    - iou [float]: iou between 2 bboxes\n",
    "    \"\"\"\n",
    "    xmin = np.max([gt_bbox[0], pred_bbox[0]])\n",
    "    ymin = np.max([gt_bbox[1], pred_bbox[1]])\n",
    "    xmax = np.min([gt_bbox[2], pred_bbox[2]])\n",
    "    ymax = np.min([gt_bbox[3], pred_bbox[3]])\n",
    "    \n",
    "    intersection = max(0, xmax - xmin + 1) * max(0, ymax - ymin + 1)\n",
    "    gt_area = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "    pred_area = (pred_bbox[2] - pred_bbox[0]) * (pred_bbox[3] - pred_bbox[1])\n",
    "    \n",
    "    union = gt_area + pred_area - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40117c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_precision_recall_curve():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fa371",
   "metadata": {},
   "source": [
    "To do so, you will first have to create the Precision-Recall (PR) curve. Once this curve is created, you need to create the smoothed version as discussed in the lesson. Finally you can use this smoothed version to calculate the mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23691bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "with open('data/predictions.json', 'r') as f:\n",
    "    preds = json.load(f)\n",
    "    \n",
    "with open('data/ground_truths.json', 'r') as f:\n",
    "        gts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb13c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IMPLEMENT THIS SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a070e",
   "metadata": {},
   "source": [
    "You also have to create a visualization of the PR and smoothed PR curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b26900",
   "metadata": {},
   "source": [
    "Make sure to check the `Desktop` to see your visualization when running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6be6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(output):\n",
    "    round_output = np.round(output * 1e4) / 1e4\n",
    "    assert round_output == 0.7286, 'Something is wrong with the mAP calculation'\n",
    "    print('mAP calculation is correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results(mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80f569",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd346f",
   "metadata": {},
   "source": [
    "To create the PR curve, you need to sort the predictions based on their confidence score and calculate the precision and recall for each subset of the predictions, as explained in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90821457",
   "metadata": {},
   "source": [
    "To make your life easier, you can hard code the smoothed PR curve based on the PR curve, but \n",
    "you should think of a scripted version of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231646a",
   "metadata": {},
   "source": [
    "References\n",
    "* [1] Everingham, M., et al., The PASCAL Visual Object Classes (VOC) Challenge. International Journal of Computer Vision. 88:303â€“338. 2010. [doi:10.1007/s11263-009-0275-4](https://doi.org/10.1007/s11263-009-0275-4).\n",
    "\n",
    "* [2] Padilla, R., et al., A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit. Electronics. 10(3):279. [doi:10.3390/electronics10030279](https://www.mdpi.com/2079-9292/10/3/279).\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [A Coder's Guide to IoU, Non-Max Suppression and Mean Average Precision by Vijayabhaskar J. | Medium](https://vijayabhaskar96.medium.com/practitioners-guide-to-iou-non-max-suppression-and-mean-average-precision-e09de73a2bd8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

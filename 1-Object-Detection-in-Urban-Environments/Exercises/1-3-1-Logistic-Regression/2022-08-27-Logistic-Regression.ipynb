{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b73b05",
   "metadata": {},
   "source": [
    "# Exercise 1.3.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f1c9a",
   "metadata": {},
   "source": [
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86bbb0",
   "metadata": {},
   "source": [
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bfcb4",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d7a91",
   "metadata": {},
   "source": [
    "In this exercise we will implement the following functions:\n",
    "* `softmax`: computes the softmax (normalised exponential function) of a input tensor;\n",
    "* `cross_entropy`: calculates the cross-entropy loss between one-hot encoded prediction and ground truth vectors;\n",
    "* `model`: logistic regression algorithm;\n",
    "* `accuracy`: calculates the accuracy between a set of predictions and corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4df85",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577cbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc3c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from tensorflow.keras import utils\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd57c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e60cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1b4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07119181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e358a4c",
   "metadata": {},
   "source": [
    "### 1.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c68d5",
   "metadata": {},
   "source": [
    "The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) is a generalisation of the sigmoid [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to multiple dimensions. In machine learning, particularly for logistic regression, the softmax function $\\phi$ acts as a decision boundary applied to multi-class datasets, computing the probability of each observation $x_{i}$ belonging to one of $j = i,...,k$ class labels (assuming an independent relationship between the classes),\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P\\left(y=j \\ \\vert \\ z_{i}\\right) = \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}\\mathcal{e}^{z_{k}^{i}}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "The input $z$ is defined to be\n",
    "$$\n",
    "\\begin{align}\n",
    "    z &= w_{0}x_{0} + w_{1}x_{1} + \\ldots + w_{m}x_{m} = \\sum_{i=0}^{m} w_{i}x_{i} = \\mathrm{w}^{\\top}\\mathrm{x}.\n",
    "    \\end{align}\n",
    "$$\n",
    "such that $\\mathrm{w}$ is the weight vector, $\\mathrm{x}$ is the feature vector belonging to a single training observation, and $w_{0}$ is the bias unit.\n",
    "\n",
    "The softmax function computes the probability for each class $P\\left(y=j \\vert x_{i}; w_{j}\\right)$, then a correction step is applied to the predictions during training using a cost function that minimises the cross-entropy over the training set observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565378ab",
   "metadata": {},
   "source": [
    "##### Note on numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba080c26",
   "metadata": {},
   "source": [
    "Exponentiation in Python can be a problem for larger numbers. A Numpy `float64` value can represent a maximal number on the order of $10^{308}$, but with exponentiation in the softmax function it is possible to overshoot this number, even for fairly modest-sized inputs (as pointed out in [this](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) post by E. Bendersky).\n",
    "\n",
    "To handle this, we can normalise the inputs using an arbitrary constant $C$ and moving it into the exponent to obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{C\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}C\\mathcal{e}^{z_{k}^{i}}} = \\frac{\\mathcal{e}^{z_{i} + \\mathrm{log}\\left(C\\right)}}{\\sum_{j=0}^{k}C\\mathcal{e}^{z_{k}^{i} + \\mathrm{log}\\left(C\\right)}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Replacing $\\mathrm{log}\\left(C\\right)$ with another arbitrary constant $D$, we can then select a value for $D$ as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    D = -max\\left(x_{1}, x_{2},\\ldots,x_{N}\\right) \n",
    "    \\end{align}\n",
    "$$\n",
    "such that all input observations $x$ will be shifted towards zero with _negative_ values. Because of this, we can better avoid NaNs as negatives with large exponents \"saturate\" to zero rather than infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80545f",
   "metadata": {},
   "source": [
    "### 1.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93022444",
   "metadata": {},
   "source": [
    "In machine learning, [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) is often used as a loss function computed between two probability distributions. Given a set of predictions $q_{i}$ and corresponding true probability values $p_{i}$ we can compute the cross-entropy loss, i.e., _log loss_ [1],\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H\\left(p,q\\right) = -\\sum_{i}{p_i}\\mathrm{log}q_i = -ylog\\hat{y} - \\left(1-y\\right)\\mathrm{log}\\left(1-\\hat{y}\\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "which serves as a measure of dissimilarity between $p$ and $q$.\n",
    "\n",
    "For a set of $n = 1,\\ldots,N$ training observations, we can compute the average of the loss function over all observations such that $H\\left(p,q\\right)$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    J(\\mathrm{w}) = \\frac{1}{N}\\sum_{n=1}^{N}H\\left(p_{n},q_{n}\\right) \n",
    "    = -\\frac{1}{N}\\sum_{n=1}^{N} \\begin{bmatrix} y_{n}\\mathrm{log}\\hat{y}_{n} + \\left(1-y_{n}\\right)\\mathrm{log}\\left(1-\\hat{y}_{n}\\right) \\end{bmatrix}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ee29e",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c6033",
   "metadata": {},
   "source": [
    "The cross-entropy loss function is defined for a training sample $x_{i}$ belonging to class $j$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(x, y; w\\right) &= H\\left(y, \\hat{y}\\right) = \\sum_{j} y_{j}\\mathrm{log}\\hat{y}_{j} = -\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k} \\mathcal{e}^{w_{j}^{\\top}x_{i}}}\n",
    "    \\end{align}\n",
    "$$\n",
    "where $y$ denotes the [one-hot](https://en.wikipedia.org/wiki/One-hot) encoded vector and $\\hat{y}$ denotes the probability distribution $h\\left(x_{i}\\right)$.\n",
    "\n",
    "The cross-entropy loss function for all observations $\\left(\\mathrm{X}_{i}, \\mathrm{Y}_{i}\\right)_{i=1}^{N}$ is then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(\\mathrm{X}, \\mathrm{Y}; \\mathrm{w}\\right) = -\\sum_{i=1}^{N}\\sum_{j=1}^{k} I\\left[y_{i} = j\\right]\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k}\\mathcal{e}^{w_{j}^{\\top} x_{i}}}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83917bf",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37076f1c",
   "metadata": {},
   "source": [
    "### 2.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5d459",
   "metadata": {},
   "source": [
    "In this exercise, you have to implement 4 different functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8128d23",
   "metadata": {},
   "source": [
    "* `softmax`: compute the softmax of a vector. This function takes as input a tensor and outputs a discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b539cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2c9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, stable=False):\n",
    "    \"\"\"Returns the softmax probability distribution.\n",
    "    \n",
    "    :param logits: a 1xN tf.Tensor of logits.\n",
    "    :param stable: optional, flag indicating whether\n",
    "        or not to normalise the input data.\n",
    "    returns: soft_logits, a 1xN tf.Tensor of real \n",
    "        values in range (0,1) that sum up to 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(logits, tf.Tensor)\n",
    "    if stable:\n",
    "        logits = tf.subtract(logits, tf.reduce_max(logits))\n",
    "    soft_logits = tf.math.exp(logits)\n",
    "    soft_logits /= tf.math.reduce_sum(soft_logits)\n",
    "    return soft_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb09b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-29 13:30:32.785538: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-08-29 13:30:32.785899: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "       0.10902364, 0.29635698])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=7 predictions\n",
    "x = [1.0, 2.0, 3.0, 1.0, 2.0, 2.0, 3.0]\n",
    "### Converting to tf.Tensor object\n",
    "x = tf.constant(x, dtype=tf.float64)\n",
    "### Computing the softmax function and printing results as Numpy array\n",
    "x_scaled = softmax(x)\n",
    "x_scaled.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1462cd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([nan, nan, nan])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=3 large values (without normalising)\n",
    "x_large = tf.constant([1000, 2000, 3000], dtype=tf.float64)\n",
    "softmax(x_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f27e893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=3 large values (with normalising)\n",
    "x_large = tf.constant([1000, 2000, 3000], dtype=tf.float64)\n",
    "softmax(x_large, stable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068bcd7",
   "metadata": {},
   "source": [
    "**Note**: this output isn't very ideal either, since the softmax function does not typically result in a zero value. However, for very large numbers, we are expecting a result extremely close to zero anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc9e35",
   "metadata": {},
   "source": [
    "### 2.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ba4a0",
   "metadata": {},
   "source": [
    "* `cross_entropy`: calculate the cross entropy loss given a vector of predictions (after softmax) and a vector of ground truth (one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0b187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974d2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "340e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(scaled_logits, one_hot, use_numpy=True):\n",
    "    \"\"\"Returns the cross-entropy loss.\n",
    "    \n",
    "    :param scaled_logits: an NxC tf.Tensor of scaled softmax\n",
    "        distribution values, [n_samples x n_classes].\n",
    "    :param one_hot: an NxC tf.Tensor of one-hot encoded \n",
    "        ground truth labels, [n_samples x n_classes].\n",
    "    :param use_numpy: optional, uses  Numpy multidimensional\n",
    "        array indexing on type-casted tf.experimental.numpy\n",
    "        ndarrays, uses boolean masking if False.\n",
    "    :returns: loss, a 1x1 tf.Tensor with cross-entropy loss. \n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(scaled_logits, tf.Tensor)\n",
    "    assert isinstance(one_hot, tf.Tensor)\n",
    "    if use_numpy:\n",
    "        n_samples = one_hot.shape[0]\n",
    "        class_labels = tf.math.argmax(one_hot, axis=1)\n",
    "        preds = tnp.asarray(scaled_logits)[range(n_samples), class_labels]\n",
    "        log_likelihood = -tf.math.log(preds)\n",
    "    else:\n",
    "        n_samples = one_hot.shape[0]\n",
    "        # For each sample, pick the probability value from the distribution\n",
    "        # that corresponds to the true class label\n",
    "        preds = tf.boolean_mask(scaled_logits, one_hot)\n",
    "        # Taking the negative log-likelihood\n",
    "        log_likelihood = -tf.math.log(preds)\n",
    "    # Normalising by the sample size\n",
    "    loss = tf.math.reduce_sum(log_likelihood) / n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72dd809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our ground-truth labels and using one-hot encoding\n",
    "y = tf.constant([2.0, 2.0, 3.0, 0.0, 2.0, 1.0, 3.0])\n",
    "y_one_hot = tf.constant(tf.keras.utils.to_categorical(y))\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac494019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 4), dtype=float64, numpy=\n",
       "array([[0.04010756, 0.04010756, 0.04010756, 0.04010756],\n",
       "       [0.10902364, 0.10902364, 0.10902364, 0.10902364],\n",
       "       [0.29635698, 0.29635698, 0.29635698, 0.29635698],\n",
       "       [0.04010756, 0.04010756, 0.04010756, 0.04010756],\n",
       "       [0.10902364, 0.10902364, 0.10902364, 0.10902364],\n",
       "       [0.10902364, 0.10902364, 0.10902364, 0.10902364],\n",
       "       [0.29635698, 0.29635698, 0.29635698, 0.29635698]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating pseudo-batched data by repeating 'predictions'\n",
    "X_scaled = tf.stack([x_scaled] * 4, axis=1)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5756b",
   "metadata": {},
   "source": [
    "**Note**: this data does not make any sense in the scheme of this problem, we are simply the output of our cross-entropy loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a87f187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=2.216190530018549>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the cross-entropy loss function with N=1 batch\n",
    "loss = cross_entropy(X_scaled, y_one_hot)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf756c0e",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa6d04",
   "metadata": {},
   "source": [
    "* `model`: takes a batch of images (stack of images along the first dimensions) and feeds it through the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b8cf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92083e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    \"\"\"Performs one step of the logistic regression model.\n",
    "    \n",
    "    :param X: tf.Tensor object, a training observation\n",
    "        i.e., a single HxWx3 RGB image.\n",
    "    :param W: tf.Tensor object, the weight vector.\n",
    "    :param b: the bias term, tf.Tensor-like object.\n",
    "    returns: tf.Tensor, the softmax probability distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(X, tf.Tensor)\n",
    "    assert isinstance(W, tf.Variable)\n",
    "    assert isinstance(b, tf.Variable)\n",
    "    # Compute the product between flattened input and weight vectors\n",
    "    Z = tf.matmul(tf.reshape(X, shape=(-1, W.shape[0])), W)\n",
    "    # Add the bias term\n",
    "    Z += b\n",
    "    # Return the softmax probabilities P\n",
    "    return softmax(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01700ab",
   "metadata": {},
   "source": [
    "### 2.4. Prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aca75b",
   "metadata": {},
   "source": [
    "* `accuracy`: given a vector of predictions and a vector of ground truth, calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06181bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf1feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Calculates the average correct predictions.\n",
    "\n",
    "    :param y_hat: tf.Tensor, NxC tensor-like object of \n",
    "        models predictions [n_samples x n_classes].\n",
    "    :param y: tf.Tensor, N-dimensional tensor of\n",
    "        ground truth class labels (not one-hot encoded).\n",
    "    returns: acc, a 1x1 scalar tf.Tensor-like object\n",
    "        with the accuracy score (correct / total predictions).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(y, tf.Tensor) and isinstance(y_hat, tf.Tensor)\n",
    "    # Get predicted labels with highest probabilities\n",
    "    y_preds = tf.cast(tf.math.argmax(y_hat, axis=1), dtype=y.dtype)\n",
    "    # Get number of correct predictions\n",
    "    n_correct = tf.math.count_nonzero(tf.cast(tf.math.equal(y_preds, y), dtype=tf.int32))\n",
    "    # Compute average correct predictions\n",
    "    acc = n_correct / y_hat.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953a203",
   "metadata": {},
   "source": [
    "### 2.5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261ddb8",
   "metadata": {},
   "source": [
    "We will check the above functions against the provided test values given by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff92e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c8a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_softmax(func):\n",
    "    logits = tf.constant([[0.5, 1.0, 2.0, 0.3, 4.0]])\n",
    "    tf_soft = tf.nn.softmax(logits)\n",
    "    soft = func(logits)\n",
    "    l1_norm = tf.norm(tf_soft - soft, ord=1)\n",
    "    assert l1_norm < 1e-5, 'Softmax calculation is wrong'\n",
    "    print('Softmax implementation is correct!')\n",
    "\n",
    "\n",
    "def check_ce(func):\n",
    "    logits = tf.constant([[0.5, 1.0, 2.0, 0.3, 4.0]])\n",
    "    scaled_logits = tf.nn.softmax(logits)\n",
    "    one_hot = tf.constant([[0, 0, 0, 0, 1.0]])\n",
    "    tf_ce = tf.nn.softmax_cross_entropy_with_logits(one_hot, logits)\n",
    "    ce = func(scaled_logits, one_hot)\n",
    "    l1_norm = tf.norm(tf_ce - ce, ord=1)\n",
    "    assert l1_norm < 1e-5, 'CE calculation is wrong'\n",
    "    print('CE implementation is correct!')\n",
    "\n",
    "\n",
    "def check_model(func):\n",
    "    # only check the output size here\n",
    "    X = tf.random.uniform([28, 28, 3])\n",
    "    num_inputs = 28*28*3\n",
    "    num_outputs = 10\n",
    "    W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs),\n",
    "                                    mean=0, stddev=0.01))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    out = func(X, W, b)\n",
    "    assert out.shape == (1, 10), 'Model is wrong!'\n",
    "    print('Model implementation is correct!')\n",
    "\n",
    "\n",
    "def check_acc(func):\n",
    "    y_hat = tf.constant([[0.8, 0.2, 0.5, 0.2, 5.0], [0.8, 0.2, 0.5, 0.2, 5.0]]) \n",
    "    y = tf.constant([4, 1])\n",
    "    acc = func(y_hat, y)\n",
    "    assert acc == tf.cast(tf.constant(0.5), dtype=acc.dtype), 'Accuracy calculation is wrong!'\n",
    "    print('Accuracy implementation is correct!') \n",
    "\n",
    "    \n",
    "def compute_ce(func, use_numpy):\n",
    "    logits = tf.constant([[0.5, 1.0, 2.0, 0.3, 4.0]])\n",
    "    scaled_logits = tf.nn.softmax(logits)\n",
    "    one_hot = tf.constant([[0, 0, 0, 0, 1.0]])\n",
    "    ce = func(scaled_logits, one_hot, use_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9fcd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the `softmax` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c2884bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "check_softmax(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a5cf15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30db7680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "check_ce(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d48415f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012832483039999997"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing average execution time using `tf.boolean_mask`\n",
    "timeit.timeit(lambda: compute_ce(cross_entropy, use_numpy=False), number=1000) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd5e330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018046973149999995"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing average execution time using Numpy indexing\n",
    "timeit.timeit(lambda: compute_ce(cross_entropy, use_numpy=True), number=1000) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3876d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the logistic regression `model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a12ddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f7ced75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the `accuracy` scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b52e89a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "check_acc(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abc791",
   "metadata": {},
   "source": [
    "## 3. Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16457d23",
   "metadata": {},
   "source": [
    "##### Alternatives\n",
    "* Use `tf.nn.softmax_cross_entropy_with_logits` instead of manually-computing the softmax and cross-entropy loss functions;\n",
    "* Use `tf.boolean_mask` instead of Numpy multidimensional array indexing in `cross_entropy` to \"mask\" correct class prediction probabilities;\n",
    "* Regularisation by multiplying an `alpha` hyperparameter with the product of the cross-entropy and L2 weight vector losses;\n",
    "* Perform maximum likelihood estimation and optimisation using the `tf.train.GradientDescentOptimizer.mimimize(loss)` function.\n",
    "\n",
    "##### Extensions of task\n",
    "* Implement a `fit` function that performs the gradient descent optimisation over a number of epochs;\n",
    "* Alternatively, use a `tf.Session` to iterate over model computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340a946",
   "metadata": {},
   "source": [
    "## 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ad1d6",
   "metadata": {},
   "source": [
    "- [ ] Run model on actual training data;\n",
    "- [ ] Use built-in TensorFlow methods for further performance optimisations;\n",
    "- [ ] Encapsulate current model with `fit` function and perform mini-batched or stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088d60a",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This assignment was prepared by Thomas Hossler and Michael Virgo et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "References\n",
    "* [1] Ji, S. Xie, Y. Logistic Regression: From Binary to Multi-Class. http://people.tamu.edu/~sji/classes/LR.pdf\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [Softmax Regression and How is it Related to Logistic Regression? | KDnuggets](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)\n",
    "* [The Softmax function and its derivative | E. Bendersky](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
    "* [Softmax and Cross Entropy Loss | P. Dahal](https://deepnotes.io/softmax-crossentropy)\n",
    "* [Multinomial Regression with TensorFlow | YouTube](https://www.youtube.com/watch?v=2JiXktBn_2M)\n",
    "* [Logistic regression 5.2: Multiclass - Softmax regression | YouTube](https://www.youtube.com/watch?v=hYBwBmojXoU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
